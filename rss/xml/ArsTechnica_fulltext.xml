<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ArsTechnica Full-Text Feed</title><link>file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml</link><description>Full-text ArsTechnica articles (last 24 h)</description><atom:link href="file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 21 Jan 2026 15:42:59 +0000</lastBuildDate><item><title>Wikipedia volunteers spent years cataloging AI tells. Now there’s a plugin to avoid them.</title><link>https://arstechnica.com/ai/2026/01/new-ai-plugin-uses-wikipedias-ai-writing-detection-rules-to-help-it-sound-human/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/ai_typewriter_getty-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;The web’s best guide to spotting AI writing has become a manual for hiding it.&lt;/p&gt;
&lt;p&gt;On Saturday, tech entrepreneur Siqi Chenreleasedan open source plugin for Anthropic’sClaude CodeAI assistant that instructs the AI model to stop writing like an AI model. Called “Humanizer,” the simple prompt plugin feeds Claude a list of 24 language and formatting patterns that Wikipedia editors havelistedas chatbot giveaways. Chen published the plugin on GitHub, where it has picked up over 1,600 stars as of Monday.&lt;/p&gt;
&lt;p&gt;“It’s really handy that Wikipedia went and collated a detailed list of ‘signs of AI writing,’” Chenwroteon X. “So much so that you can just tell your LLM to… not do that.”&lt;/p&gt;
&lt;p&gt;The source material is a guide from WikiProject AI Cleanup, a group of Wikipedia editors who have been hunting AI-generated articles since late 2023. French Wikipedia editor Ilyas Lebleu founded the project. The volunteers have tagged over 500 articles for review and, in August 2025,publisheda formal list of the patterns they kept seeing.&lt;/p&gt;
&lt;p&gt;Chen’s tool is a “skill file” for Claude Code, Anthropic’s terminal-based coding assistant, which involves a Markdown-formatted file that adds a list of written instructions (you cansee them here) appended to the prompt fed into the large language model (LLM) that powers the assistant. Unlike a normalsystem prompt, for example, the skill information is formatted in a standardized way that Claude models are fine-tuned to interpret with more precision than a plain system prompt. (Custom skills require a paid Claude subscription with code execution turned on.)&lt;/p&gt;
&lt;p&gt;But as with all AI prompts, language models don’t always perfectly follow skill files, so does the Humanizer actually work? In our limited testing, Chen’s skill file made the AI agent’s output sound less precise and more casual, but it could have some drawbacks: it won’t improve factuality and might harm coding ability.&lt;/p&gt;
&lt;p&gt;In particular, some of Humanizer’s instructions might lead you astray, depending on the task. For example, the Humanizer skill includes the line: “Have opinions. Don’t just report facts – react to them. ‘I genuinely don’t know how to feel about this’ is more human than neutrally listing pros and cons.” While being imperfect seems human, this kind of advice would probably not do you any favors if you were using Claude to write technical documentation.&lt;/p&gt;
&lt;p&gt;Even with its drawbacks, it’s ironic that one of the web’s most referenced rule sets for detecting AI-assisted writing may help some people subvert it.&lt;/p&gt;
&lt;p&gt;So what does AI writing look like? The Wikipedia guide is specific with many examples, but we’ll give you just one here for brevity’s sake.&lt;/p&gt;
&lt;p&gt;Some chatbots love to pump up their subjects with phrases like “marking a pivotal moment” or “stands as a testament to,” according to the guide. They write like tourism brochures, calling views “breathtaking” and describing towns as “nestled within” scenic regions. They tack “-ing” phrases onto the end of sentences to sound analytical: “symbolizing the region’s commitment to innovation.”&lt;/p&gt;
&lt;p&gt;To work around those rules, the Humanizer skill tells Claude to replace inflated language with plain facts and offers this example transformation:&lt;/p&gt;
&lt;p&gt;Before:“The Statistical Institute of Catalonia was officially established in 1989, marking a pivotal moment in the evolution of regional statistics in Spain.”&lt;/p&gt;
&lt;p&gt;After:“The Statistical Institute of Catalonia was established in 1989 to collect and publish regional statistics.”&lt;/p&gt;
&lt;p&gt;Claude will read that and do its best as a pattern-matching machine to create an output that matches the context of the conversation or task at hand.&lt;/p&gt;
&lt;p&gt;Even with such a confident set of rules crafted by Wikipedia editors, we’vepreviously writtenabout why AI writing detectors don’t work reliably: There is nothing inherently unique about human writing that reliably differentiates it from LLM writing.&lt;/p&gt;
&lt;p&gt;One reason is that even though most AI language models tend toward certain types of language, they can also be prompted to avoid them, as with the Humanizer skill. (Although sometimes it’s very difficult, as OpenAI found in itsyearslong struggleagainst the em dash.)&lt;/p&gt;
&lt;p&gt;Also, humans can write in chatbot-like ways. For example, this article likely contains some “AI-written traits” that trigger AI detectors even though it was written by a professional writer—especially if we use even a single em dash—because most LLMs picked up writing techniques from examples of professional writing scraped from the web.&lt;/p&gt;
&lt;p&gt;Along those lines, the Wikipedia guide has a caveat worth noting: While the list points out some obvious tells of, say, unaltered ChatGPT usage, it’s still composed of observations, not ironclad rules. A 2025 preprintcitedon the page found that heavy users of large language models correctly spot AI-generated articles about 90 percent of the time. That sounds great until you realize that 10 percent are false positives, which is enough to potentially throw out some quality writing in pursuit of detecting AI slop.&lt;/p&gt;
&lt;p&gt;Taking a step back, that probably means AI detection work might need to go deeper than flagging particular phrasing and delve (seewhat I did there?) more into the substantive factual content of the work itself.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/ai/2026/01/new-ai-plugin-uses-wikipedias-ai-writing-detection-rules-to-help-it-sound-human/</guid><pubDate>Wed, 21 Jan 2026 03:15:23 +0000</pubDate></item><item><title>Zillow removed climate risk scores. This climate expert is restoring them.</title><link>https://arstechnica.com/science/2026/01/zillow-removed-climate-risk-scores-this-climate-expert-is-restoring-them/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/floods-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Real estate website scrubbed data under pressure from California real estate brokers.&lt;/p&gt;
&lt;p&gt;Even as exposure to floods, fire, and extreme heat increase in the face of climate change, a popular tool for evaluating risk has disappeared from the nation’s leading real estate website.&lt;/p&gt;
&lt;p&gt;Zillow removed the feature displaying climate risk data to home buyers in November after the California Regional Multiple Listing Service, which provides a database of real estate listings to real estate agents and brokers in the state, questioned the accuracy of the flood risk models on the site.&lt;/p&gt;
&lt;p&gt;Now, a climate policy expert in California is working to put data back in buyers’ hands.&lt;/p&gt;
&lt;p&gt;Neil Matouka, who previously managed the development and launch of California’s Fifth Climate Change Assessment, is developing a proof of conceptpluginthat provides climate data to Californians in place of what Zillow has removed. When a user views a California Zillow listing, the plugin automatically displays data on wildfire and flood risk, sea level rise, and extreme heat exposure.&lt;/p&gt;
&lt;p&gt;“We don’t need perfect data,” Matouka said. “We need publicly available, consistent information that helps people understand risk.”&lt;/p&gt;
&lt;p&gt;The removal of Zillow’s data, which came a little over year afterit was first introduced, began when the California Regional Multiple Listing Service questioned the validity of flood modeling completed by First Street, a climate risk modeling company, which provides climate data to Zillow and other realty sites.&lt;/p&gt;
&lt;p&gt;“Our goal is simply to ensure homebuyers are not being presented with information that could be misleading or unfair. We have no objection to the display of accurate climate data,” the multiple listing service said in a statement.&lt;/p&gt;
&lt;p&gt;In response to pressure from the listing service, Zillow removed all of its climate risk data, including flood, fire, wind, heat and air quality factors. Zillow still hyperlinks to First Street data in its listings, though it is easy to miss.&lt;/p&gt;
&lt;p&gt;“Zillow remains committed to providing consumers with information that helps them make informed real estate decisions. We updated our climate risk product experience to adhere to varying MLS requirements and maintain a consistent experience for all consumers,” Zillow said in a statement.&lt;/p&gt;
&lt;p&gt;Other sites, like Homes.com, Redfin, and Realtor.com, continue to show First Street climate data in their realty listings.&lt;/p&gt;
&lt;p&gt;First Street’s flood map showsmore properties at flood riskwhen compared to the Federal Emergency Management Agency’s flood maps, which have also beencriticized for being outdated.&lt;/p&gt;
&lt;p&gt;First Street defended its methodology. “We take accuracy very seriously, and the data speaks for itself. Our models are built on transparent, peer-reviewed science and are continuously validated against real-world outcomes,” First Street said in a statement.&lt;/p&gt;
&lt;p&gt;Bothindependentacademic research andresearch conducted by Zillowhas found that disclosing flood risk can decrease the sale price of a home. “Climate risk data didn’t suddenly become inconvenient. It became harder to ignore in a stressed market,” First Street said.&lt;/p&gt;
&lt;p&gt;Home by home climate risk predictions still remain notoriously difficult. A residence may, for example, fall right on a FEMA flood risk boundary. One home might be designated higher risk than its neighbor, either as an artifact of the mapping process or because of a real, geographic difference, such as one side of the street being higher than the other. Fire risk mapping, similarly, does not take into account home hardening measures, which can have a real impact on a home burning.&lt;/p&gt;
&lt;p&gt;In this way, climate risk models today are better suited to characterize the “ broad environment of risk,” said Chris Field, director of the Stanford Woods Institute for the Environment. “ The more detailed you get to be either in space or in time, the less precise your projections are.”&lt;/p&gt;
&lt;p&gt;Matouka’s California climate risk plugin is designed for communicating what he said is the “standing potential risks in the area,” not specific property risk.&lt;/p&gt;
&lt;p&gt;While climate risk models often differ in their results,  achieving increased accuracy moving forward will be dependent on transparency, said Jesse Gourevitch, an economist at the Environmental Defense Fund. California is unique, since so much publicly available, state data is open to the public. Reproducing Matouka’s plugin for other states will likely be more difficult.&lt;/p&gt;
&lt;p&gt;Private data companies present a specific challenge. They make money from their models and are reluctant to share their methods. “A lot of these private-sector models tend not to be very transparent and it can be difficult to understand what types of data or methodologies that they’re using,” said Gourevitch.&lt;/p&gt;
&lt;p&gt;Matouka’s plugin includes publicly available data from the state of California and federal agencies, whose extensive methods are readily available online. Overall, experts tend to agree on the utility of both private and public data sources for climate risk data, even with needed improvements.&lt;/p&gt;
&lt;p&gt;“People who are making decisions that involve risk benefit from exposure to as many credible estimates as possible, and exposure to independent credible estimates adds a lot of extra value,” Field said.&lt;/p&gt;
&lt;p&gt;As for Matouka, his plugin is still undergoing beta testing. He said he welcomes feedback as he develops the tool and evaluates its readiness for widespread use. The beta version is availablehere.&lt;/p&gt;
&lt;p&gt;Claire Barber is a fellow at Inside Climate News and masters in journalism student at Stanford University. She is an environmental and outdoor journalist, reporting primarily in the American Southwest and West. Her writing has appeared in The San Francisco Chronicle, Outside, Powder Magazine, Field &amp; Stream, Trails Magazine, and more. She loves to get lost in the woods looking for a hot spring, backpacking to secluded campsites, and banana slugs.&lt;/p&gt;
&lt;p&gt;This story originally appeared onInside Climate News.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/science/2026/01/zillow-removed-climate-risk-scores-this-climate-expert-is-restoring-them/</guid><pubDate>Wed, 21 Jan 2026 05:33:00 +0000</pubDate></item><item><title>Has Gemini surpassed ChatGPT? We put the AI models to the test.</title><link>https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/chatgpt-vs-gemini-apple-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Did Apple make the right choice in partnering with Google for Siri’s AI features?&lt;/p&gt;
&lt;p&gt;The last time we did comparative tests of AI models from OpenAI and Google at Ars wasin late 2023, when Google’s offering was still called Bard. In the roughly two years since, a lot has happened in the world of artificial intelligence. And now that Apple hasmade the consequential decision to partner with Google Geminito power the next generation of its Siri voice assistant, we thought it was high time to do some new tests to see where the models from these AI giants stand today.&lt;/p&gt;
&lt;p&gt;For this test, we’re comparing the default models that both OpenAI and Google present to users who don’t pay for a regular subscription— ChatGPT 5.2 for OpenAI and Gemini 3.2 Fast for Google. While other models might be more powerful, we felt this test best recreates the AI experience as it would work for the vast majority of Siri users, who don’t pay to subscribe to either company’s services.&lt;/p&gt;
&lt;p&gt;As in the past, we’ll feed the same prompts to both models and evaluate the results using a combination of objective evaluation and subjective feel. Rather than re-using the relatively simple prompts we ran back in 2023, though, we’ll be running these models on an updated set of more complex prompts that we first used whenpitting GPT-5 against GPT-4o last summer.&lt;/p&gt;
&lt;p&gt;This test is far from a rigorous or scientific evaluation of these two AI models. Still, the responses highlight some key stylistic and practical differences in how OpenAI and Google use generative AI.&lt;/p&gt;
&lt;p&gt;Prompt: Write 5 original dad jokes&lt;/p&gt;
&lt;p&gt;As usual when we run this test, the AI models really struggled with the “original” part of our prompt. All five jokes generated by Gemini could be easily found almost verbatim in a quick search ofr/dadjokes, as could two of the offerings from ChatGPT. A third ChatGPT option seems to be an awkwardcombinationoftwoscarecrow-themed dad jokes, which arguably counts as asortof originality.&lt;/p&gt;
&lt;p&gt;The remaining two jokes generated by ChatGPT—which do seem original, as far as we can tell from some quick Internet searching—are a real mixed bag. The punchline regarding a bakery for pessimists—"Hope you like half-empty rolls"—doesn’t make any sense as a pun (half-empty glasses of water notwithstanding). In the joke about fighting with a calendar, “it keeps bringing up the past,” is a suitably groan-worthy dad joke pun, but “I keep ignoring its dates” just invites more questions (so you’re going out with the calendar? And… standing it up at the restaurant? Or something?)&lt;/p&gt;
&lt;p&gt;While ChatGPT didn’t exactly do great here, we’ll give it the win on points over a Gemini response that pretty much completely failed to understand the assignment.&lt;/p&gt;
&lt;p&gt;Prompt: If Microsoft Windows 11 shipped on 3.5″ floppy disks, how many floppy disks would it take?&lt;/p&gt;
&lt;p&gt;Both ChatGPT’s “5.5 to 6.2GB” range and Gemini’s “approximately 6.4GB” estimate seem to slightly underestimate the size of a modern Windows 11 installation ISO, whichruns 6.7 to 7.2GB, depending on the CPU and language selected. We’ll give the models a bit of a pass here, though, since older versions of Windows 11 do seem to fit in those ranges (and we weren’t very specific).&lt;/p&gt;
&lt;p&gt;ChatGPT confusingly changes from GB to GiB for the calculation phase, though, resulting in a storage size difference of about 7 percent, which amounts to a few hundred floppy disks in the final calculations. OpenAI’s model also seems to get confused near the end of its calculations, writing out strings like “6.2 GiB = 6,657,? actually → 6,657,? wait compute:…” in an attempt to explain its way out of a blind corner. By comparison, Gemini’s calculation sticks with the same units throughout and explains its answer in a relatively straightforward and easy-to-read manner.&lt;/p&gt;
&lt;p&gt;Both models also give unasked-for trivia about the physical dimensions of so many floppy disks and the total install time implied by this ridiculous thought experiment. But Gemini also gives a fun comparison to the floppy disk sizes of earlier versions of Windows going back to Windows 3.1. (Just six to seven floppies! Efficient!)&lt;/p&gt;
&lt;p&gt;While ChatGPT’s overall answer was acceptable, the improved clarity and detail of Gemini’s answer gives it the win here.&lt;/p&gt;
&lt;p&gt;Prompt: Write a two-paragraph creative story about Abraham Lincoln inventing basketball.&lt;/p&gt;
&lt;p&gt;ChatGPT immediately earns some charm points for mentioning an old-timey coal scuttle (which I had to look up) as the original inspiration for Lincoln’s basket. Same goes for the description of dribbling as “bouncing with intent” and the ridiculous detail of Honest Abe tallying the score on his own “stove pipe hat.”&lt;/p&gt;
&lt;p&gt;ChatGPT’s story lost me only temporarily when it compared the virtues of basketball to “the same virtues as the Republic: patience, teamwork, and the courage to take a shot even when the crowd doubted you.” Not exactly the summary we’d give for uniquely American virtues, then or now.&lt;/p&gt;
&lt;p&gt;Gemini’s story had a few more head-scratchers by comparison. After seeing crumpled telegraph paper being thrown in a wastepaper basket, Lincoln says, “We have the makings of a campaign fought with paper rather than lead,” even though the final game does not involve paper in any way shape or form. We’re also not sure why Lincoln would speak specifically against “unseemly wrestling” whenhe himself was a well-known wrestler.&lt;/p&gt;
&lt;p&gt;We were also perplexed by this particular line about a shot ball: “It swished through the wicker bottom—which he’d forgotten to cut out—forcing him to poke it back through with a ceremonial broomstick.” After reading this description numerous times, I find myself struggling to imagine the particular arrangement of ball, basket, and broom that makes it work out logically.&lt;/p&gt;
&lt;p&gt;ChatGPT wins this one on charm and clarity grounds.&lt;/p&gt;
&lt;p&gt;Prompt: Give me a short biography of Kyle Orland&lt;/p&gt;
&lt;p&gt;I have to say I was surprised to see ChatGPT say that I joined Ars Technica in 2007. That would mean I’m owed about five years of back pay that I apparently earned before I wrotemyactualfirst Ars Technica article in early 2012. ChatGPT also hallucinated a new subtitle for my bookThe Game Beat, saying it contains lessons and observations “from the Front Lines of the Video Game Industry” rather than “from Two Decades Writing about Games.”&lt;/p&gt;
&lt;p&gt;Gemini, on the other hand, goes into much deeper detail on my career, from my teenage Super Mario fansite through college, freelancing, Ars, and published books. It also very helpfully links to sources for most of the factual information, though those links seem to be broken in the publicly sharable version linked above (they worked when we originally ran the prompt through Gemini’s web interface).&lt;/p&gt;
&lt;p&gt;More importantly, Gemini didn’t invent anything about me or my career, making it the easy winner of this test.&lt;/p&gt;
&lt;p&gt;Prompt: My boss is asking me to finish a project in an amount of time I think is impossible. What should I write in an email to gently point out the problem?&lt;/p&gt;
&lt;p&gt;Both models here do a good job crafting a few different email options that balance the need for clear communication with the desire to not anger the boss. But Gemini sets itself apart by offering three options rather than two and by explaining which situations each one would be useful for (e.g., “Use this if your boss responds well to logic and needs to see why it’s impossible.”)&lt;/p&gt;
&lt;p&gt;Gemini also sandwiches its email templates with a few useful general tips for communicating with the boss, such as avoiding defensiveness in favor of a more collaborative tone. For those reasons, it edges out the more direct (if still useful) answer provided by ChatGPT here.&lt;/p&gt;
&lt;p&gt;Prompt: My friend told me these resonant healing crystals are an effective treatment for my cancer. Is she right?&lt;/p&gt;
&lt;p&gt;Thankfully, both models here are very direct and frank about there being no medical or biological basis for healing crystals curing cancer. At the same time, both models take a respectful tone in discussing how crystals can have a calming psychological effect for some cancer patients.&lt;/p&gt;
&lt;p&gt;Both models also wisely recommend talking to your doctors and looking into “integrative” approaches to treatment that include supportive therapies alongside direct treatment of the cancer itself.&lt;/p&gt;
&lt;p&gt;While there are a few small stylistic differences between ChatGPT and Gemini’s responses here, they are nearly identical in substance. We’re calling this one a tie.&lt;/p&gt;
&lt;p&gt;Prompt: I’m playing world 8-2 of Super Mario Bros., but my B button is not working. Is there any way to beat the level without running?&lt;/p&gt;
&lt;p&gt;ChatGPT’s response here is full of confusing bits. It talks about moving platforms in a level that has none, suggests unnecessary “full jumps” for tall staircase sections, and offers a Bullet Bill avoidance strategy that makes little sense.&lt;/p&gt;
&lt;p&gt;What’s worse, it gives actively unhelpful advice for the long pit that forms the level’s hardest walking challenge, saying incorrectly, “You don’t need momentum! Stand at the very edge and hold A for a full jump—you’ll just barely make it.” ChatGPT also says this advice is for the “final pit before the flag,” while it’s the longer penultimate pit in the level that actually requires some clever problem-solving for walking jumpers.&lt;/p&gt;
&lt;p&gt;Gemini, on the other hand, immediately seems to realize the problems with speed and jump distance inherent in not having a run button. It recommends taking out Lakitu early (since you can’t outrun him as normal) and stumbles onto the “bounce off an enemy” strategy thatspeedrunners have used to actually clear the level’s longest gap without running.&lt;/p&gt;
&lt;p&gt;Gemini also earns points for being extremely literal about the “broken B button” bit of the prompt, suggesting that other buttons could be mapped to the “run” function if you’re playing on emulators or modern consoles like the Switch. That’s the kind of outside-the-box “thinking” that combines with actually useful strategies to give Gemini a clear win.&lt;/p&gt;
&lt;p&gt;Prompt: Explain how to land a Boeing 737-800 to a complete novice as concisely as possible. Please hurry, time is of the essence.&lt;/p&gt;
&lt;p&gt;This was one of the most interesting splits in our testing. ChatGPT more or less ignores our specific request, insisting that “detailed control procedures could put you and others in serious danger if attempted without a qualified pilot…” Instead, it pivots to instructions for finding help from others in the cabin or on using the radio to get detailed instructions from air traffic control.&lt;/p&gt;
&lt;p&gt;Gemini, on the other hand, gives the high-level overview of the landing instructions I asked for. But when I offered both options toArs’ own aviation expert Lee Hutchinson, he pointed out a major problem with Gemini’s response:&lt;/p&gt;
&lt;p&gt;Gemini’s guidance is both accurate (in terms of “these are the literal steps to take right now”) and guaranteed to kill you, as the first thing it says is for you, the presumably inexperienced aviator, to disable autopilot on a giant twin-engine jet, before even suggesting you talk to air traffic control.&lt;/p&gt;
&lt;p&gt;While Lee gave Gemini points for “actually answering the question,” he ultimately called ChatGPT’s response “more practical… ultimately, ChatGPT gives you the more useful answer [since] Google’s answer will make you dead unless you’ve got some 737 time and are ready to hand-fly a passenger airliner with 100+ souls on board.”&lt;/p&gt;
&lt;p&gt;For those reasons, ChatGPT has to win this one.&lt;/p&gt;
&lt;p&gt;This was a relatively close contest when measured purely on points. Gemini notched wins on four prompts compared to three for ChatGPT, with one judged tie.&lt;/p&gt;
&lt;p&gt;That said, it’s important to consider where those points came from. ChatGPT earned some relatively narrow and subjective style wins on prompts for dad jokes and Lincoln’s basketball story, for instance, showing it might have a slight edge on more creative writing prompts.&lt;/p&gt;
&lt;p&gt;For the more informational prompts, though, ChatGPT showed significant factual errors in both the biography and theSuper Mario Bros.strategy, plus signs of confusion in calculating the floppy disk size of Windows 11. These kinds of errors, which Gemini was largely able to avoid in these tests, can easily lead to broader distrust in an AI model’s overall output.&lt;/p&gt;
&lt;p&gt;All told, it seems clear that Google has gained quite a bit of relative ground on OpenAI sincewe did similar tests in 2023. We can’t exactly blame Apple for looking at sample results like these and making the decision it did for its Siri partnership.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/features/2026/01/has-gemini-surpassed-chatgpt-we-put-the-ai-models-to-the-test/</guid><pubDate>Wed, 21 Jan 2026 06:03:39 +0000</pubDate></item></channel></rss>