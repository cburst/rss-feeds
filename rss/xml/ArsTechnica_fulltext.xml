<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ArsTechnica Full-Text Feed</title><link>file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml</link><description>Full-text ArsTechnica articles (last 24 h)</description><atom:link href="file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 19 Jan 2026 22:41:10 +0000</lastBuildDate><item><title>10 things I learned from burning myself out with AI coding agents</title><link>https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/super-programmer-hes-heating-up-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Opinion: As software power tools, AI agents may make people busier than ever before.&lt;/p&gt;
&lt;p&gt;If you’ve ever used a 3D printer, you may recall the wondrous feeling when you first printed something you could have never sculpted or built yourself. Download a model file, load some plastic filament, push a button, and almost like magic, a three-dimensional object appears. But the result isn’t polished and ready for mass production, and creating a novel shape requires more skills than just pushing a button. Interestingly, today’sAI coding agentsfeel much the same way.&lt;/p&gt;
&lt;p&gt;Since November, I have usedClaude Codeand Claude Opus 4.5 through a personal Claude Max account to extensively experiment with AI-assisted software development (I have also used OpenAI’sCodexin a similar way, though not as frequently). Fifty projects later, I’ll be frank: I have not had this much fun with a computer since I learned BASIC on myApple II Pluswhen I was 9 years old. This opinion comes not as an endorsement but as personal experience: I voluntarily undertook this project, and I paid out of pocket for both OpenAI and Anthropic’s premium AI plans.&lt;/p&gt;
&lt;p&gt;Throughout my life, I have dabbled in programming as a utilitarian coder, writing small tools or scripts when needed. In my web development career, I wrote some small tools from scratch, but I primarily modified other people’s code for my needs. Since 1990, I’ve programmed in BASIC, C, Visual Basic, PHP, ASP, Perl, Python, Ruby, MUSHcode, and some others. I am not an expert in any of these languages—I learned just enough to get the job done. I have developed my own hobby games over the years using BASIC, Torque Game Engine, and Godot, so I have some idea of what makes a good architecture for a modular program that can be expanded over time.&lt;/p&gt;
&lt;p&gt;Claude Code, Codex, and Google’sGemini CLI, can seemingly perform software miracles on a small scale. They can spit out flashy prototypes of simple applications, user interfaces, and even games, but only as long as they borrow patterns from their training data. Much like a 3D printer, doing production-level work takes far more effort. Creating durable production code, managing a complex project, or crafting something truly novel still requires experience, patience, and skill beyond what today’s AI agents can provide on their own.&lt;/p&gt;
&lt;p&gt;And yet these tools have opened a world of creative potential in software that was previously closed to me, and they feel personally empowering. Even with that impression, though, I know these are hobby projects, and the limitations of coding agents lead me to believe that veteran software developers probably shouldn’t fear losing their jobs to these tools any time soon. In fact, they may become busier than ever.&lt;/p&gt;
&lt;p&gt;So far, I have created over 50 demo projects in the past two months, fueled in part by a bout of COVID that left me bedridden with a laptop and a generous 2x Claude usage cap that Anthropic put in place during the last few weeks of December. As I typed furiously all day, my wife kept asking me, “Who are you talking to?”&lt;/p&gt;
&lt;p&gt;You can see a few of the more interesting resultslistedon my personal website. Here are 10 interesting things I’ve learned from the process.&lt;/p&gt;
&lt;p&gt;Even with the best AI coding agents available today, humans remain essential to the software development process. Experienced human software developers bring judgment, creativity, and domain knowledge that AI models lack. They know how to architect systems for long-term maintainability, how to balance technical debt against feature velocity, and when to push back when requirements don’t make sense.&lt;/p&gt;
&lt;p&gt;For hobby projects like mine, I can get away with a lot of sloppiness. But for production work, having someone who understands version control, incremental backups, testing one feature at a time, and debugging complex interactions between systems makes all the difference. Knowing something about how good software development works helps a lot when guiding an AI coding agent—the tool amplifies your existing knowledge rather than replacing it.&lt;/p&gt;
&lt;p&gt;As independent AI researcher Simon Willisonwrotein a post distinguishing serious AI-assisted development from casual “vibe coding,” “AI tools amplify existing expertise. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.”&lt;/p&gt;
&lt;p&gt;With AI assistance, you don’t have to rememberhowto do everything. You just need to know what you want to do.&lt;/p&gt;
&lt;p&gt;So I like to remind myself that coding agents are software tools best used to enact human ideas, not autonomous coding employees. They arenot people(and not people replacements) no matter how the companies behind them might market them.&lt;/p&gt;
&lt;p&gt;If you think about it, everything you do on a computer was once a manual process. Programming a computer like theENIACinvolved literally making physical bits (connections) with wire on aplugboard. The history of programming has been one of increasing automation, so even though this AI-assisted leap is somewhat startling, one could think of these tools as an advancement similar to the advent of high-level languages, automated compilers and debugger tools, or GUI-based IDEs. They can automate many tasks, but managing the overarching project scope still falls to the person telling the tool what to do.&lt;/p&gt;
&lt;p&gt;And they can have rapidly compounding benefits. I’ve now used AI tools to write better tools—such as changing the source of an emulator so a coding agent can use it directly—and those improved tools are already having ripple effects. But a human must be in the loop for the best execution of my vision. This approach has kept me very busy, and contrary to some prevailing fears about peoplebecoming dumberdue to AI, I have learned many new things along the way.&lt;/p&gt;
&lt;p&gt;Like all AI models based on theTransformer architecture, the large language models (LLMs) that underpin today’s coding agents have a significant limitation: They can only reliably apply knowledge gleaned from training data, and they have a limited ability to generalize that knowledge to novel domains not represented in that data.&lt;/p&gt;
&lt;p&gt;What is training data? In this case, when building coding-flavored LLMs, AI companiesdownloadmillions of examples of software code from sources like GitHub and use them to make the AI models. Companies later specialize them for coding through fine-tuning processes.&lt;/p&gt;
&lt;p&gt;The ability of AI agents to use trial and error—attempting something and then trying again—helps mitigate the brittleness of LLMs somewhat. But it’s not perfect, and it can be frustrating to see a coding agent spin its wheels trying and failing at a task repeatedly, either because it doesn’t know how to do it or because it previously learned how to solve a problem but then forgot because the context window got compacted (more on thathere).&lt;/p&gt;
&lt;p&gt;To get around this, it helps to have the AI model take copious notes as it goes along about how it solved certain problems so that future instances of the agent can learn from them again. You also want to set ground rules in the claude.md file that the agent reads when it begins its session.&lt;/p&gt;
&lt;p&gt;This brittleness means that coding agents are almost frighteningly good at what they’ve been trained and fine-tuned on—modern programming languages, JavaScript, HTML, and similar well-represented technologies—and generally terrible at tasks on which they have not been deeply trained, such as 6502 Assembly or programming an Atari 800 game with authentic-looking character graphics.&lt;/p&gt;
&lt;p&gt;It took me five minutes to make a nice HTML5 demo with Claude but a week of torturous trial and error, plus actual systematic design on my part, to make a similar demo of an Atari 800 game. To do so, I had to use Claude Code to invent several tools, like command-line emulators andMCP servers, that allow it to peek into the operation of the Atari 800’s memory and chipset to even begin to make it happen.&lt;/p&gt;
&lt;p&gt;Due to what might poetically be called “preconceived notions” baked into a coding model’s neural network (more technically, statistical semantic associations), it can be difficult to get AI agents to create truly novel things, even if you carefully spell out what you want.&lt;/p&gt;
&lt;p&gt;For example, I spent four days trying to get Claude Code to create an Atari 800 version of my HTML gameViolent Checkers, but it had trouble because in the game’s design, the squares on the checkerboard don’t matter beyond their starting positions. No matter how many times I told the agent (and made notes in my Claude project files), it would come back to trying to center the pieces to the squares, snap them within squares, or use the squares as a logical basis of the game’s calculations when they should really just form a background image.&lt;/p&gt;
&lt;p&gt;To get around this in the Atari 800 version, I started over and told Claude that I was creating a game with a UFO (instead of a circular checker piece) flying over a field of adjacent squares—never once mentioning the words “checker,” “checkerboard,” or “checkers.” With that approach, I got the results I wanted.&lt;/p&gt;
&lt;p&gt;Why does this matter? Because with LLMs, context is everything, and in language, context changes meaning. Take the word “bank” and add the words “river” or “central” in front of it, and see how the meaning changes. In a way, words act as addresses thatunlockthe semantic relationships encoded in a neural network. So if you put “checkerboard” and “game” in the context, the model’sself-attention processlinks up a massive web of semantic associations about how checkers games should work, and that semantic baggage throws things off.&lt;/p&gt;
&lt;p&gt;A couple of tricks can help AI coders navigate around these limitations. First, avoid contaminating the context with irrelevant information. Second, when the agent gets stuck, try this prompt: “What information do you need that would let you implement this perfectly right now? What tools are available to you that you could use to discover that information systematically without guessing?” This forces the agent to identify (semantically link up) its own knowledge gaps, spelled out in the context window and subject to future action, instead of flailing around blindly.&lt;/p&gt;
&lt;p&gt;The first 90 percent of an AI coding project comes in fast and amazes you. The last 10 percent involves tediously filling in the details through back-and-forth trial-and-error conversation with the agent. Tasks that require deeper insight or understanding than what the agent can provide still require humans to make the connections and guide it in the right direction. The limitations we discussed above can also cause your project to hit a brick wall.&lt;/p&gt;
&lt;p&gt;From what I have observed over the years, larger LLMs can potentially make deeper contextual connections than smaller ones. They havemore parameters(encoded data points), and those parameters are linked in more multidimensional ways, so they tend to have a deeper map of semantic relationships. As deep as those go, it seems that human brains still have an even deeper grasp of semantic connections and can make wild semantic jumps that LLMs tend not to.&lt;/p&gt;
&lt;p&gt;Creativity, in this sense, may be when you jump from, say, basketball to how bubbles form in soap film and somehow make a useful connection that leads to a breakthrough. Instead, LLMs tend to follow conventional semantic paths that are more conservative and entirely guided by mapped-out relationships from the training data. That limits their creative potential unless the prompter unlocks it by guiding the LLM to make novel semantic connections. That takes skill and creativity on the part of the operator, which once again shows the role of LLMs as tools used by humans rather than independent thinking machines.&lt;/p&gt;
&lt;p&gt;While creating software with AI coding tools, the joy of experiencing novelty makes you want to keep adding interesting new features rather than fixing bugs or perfecting existing systems. And Claude (or Codex) is happy to oblige, churning away at new ideas that are easy to sketch out in a quick and pleasing demo (the 90 percent problem again) rather than polishing the code.&lt;/p&gt;
&lt;p&gt;Fixing bugs can also create bugs elsewhere. This is not new to coding agents—it’s a time-honored problem in software development. But agents supercharge this phenomenon because they can barrel through your code and make sweeping changes in pursuit of narrow-minded goals that affect lots of working systems. We’ve already talked about the importance of having a good architecture guided by the human mind behind the wheel above, and that comes into play here.&lt;/p&gt;
&lt;p&gt;Given the limitations I’ve described above, it’s very clear that an AI model with general intelligence—what people usually callartificial general intelligence(AGI)—is still not here. AGI would hypothetically be able to navigate around baked-in stereotype associations and not have to rely on explicit training or fine-tuning on many examples to get things right. AI companies will probably need a different architecture in the future.&lt;/p&gt;
&lt;p&gt;I’m speculating, but AGI would likely need to learn permanently on the fly—as in modify its own neural network weights—instead of relying on what is called “in-context learning,” which only persists until the context fills up and gets compacted or wiped out.&lt;/p&gt;
&lt;p&gt;In other words, you could teach a true AGI system how to do something by explanation or let it learn by doing, noting successes, and having those lessons permanently stick, no matter what is in the context window. Today’s coding agents can’t do that—they forget lessons from earlier in a long session or between sessions unless you manually document everything for them. My favorite trick is instructing them to write a long, detailed report on what happened when a bug is fixed. That way, you can point to the hard-earned solution the next time the amnestic AI model makes the same mistake.&lt;/p&gt;
&lt;p&gt;While using Claude Code for a while, it’s easy to take for granted that you suddenly have the power to create software without knowing certain programming languages. This is amazing at first, but you can quickly become frustrated that what is conventionally a very fast development process isn’t fast enough. Impatience at the coding machine sets in, and you start wanting more.&lt;/p&gt;
&lt;p&gt;But even if you do know the programming languages being used, you don’t get a free pass. You still need to make key decisions about how the project will unfold. And when the agent gets stuck or makes a mess of things, your programming knowledge becomes essential for diagnosing what went wrong and steering it back on course.&lt;/p&gt;
&lt;p&gt;After guiding way too many hobby projects through Claude Code over the past two months, I’m starting to think that most people won’t become unemployed due to AI—they will become busier than ever. Power tools allow more work to be done in less time, and the economy will demand more productivity to match.&lt;/p&gt;
&lt;p&gt;It’s almost too easy to make new software, in fact, and that can be exhausting. One project idea would lead to another, and I was soon spending eight hours a day during my winter vacation shepherding about 15 Claude Code projects at once. That’s too much split attention for good results, but the novelty of seeing my ideas come to life was addictive. In addition to the game ideas I’ve mentioned here, I made tools that scrape and search my past articles, a graphical MUD based on ZZT, a new type ofMUSH(text game) that uses AI-generated rooms, a new type ofTelnet display proxy, and a Claude Code client for the Apple II (more on that soon). I also put two AI-enabled emulators forApple IIandAtari 800on GitHub. Phew.&lt;/p&gt;
&lt;p&gt;Consider the advent of thesteam shovel, which allowed humans to dig holes faster than a team using hand shovels. It made existing projects faster and new projects possible. But think about the human operator of the steam shovel. Suddenly, we had a tireless tool that could work 24 hours a day if fueled up and maintained properly, while the human piloting it would need to eat, sleep, and rest.&lt;/p&gt;
&lt;p&gt;In fact, we may end up needing new protections for human knowledge workers using these tireless information engines to implement their ideas, much as unions rose as a response to industrial production lines over 100 years ago. Humans need rest, even when machines don’t.&lt;/p&gt;
&lt;p&gt;Will an AI system ever replace the human role here? Even if AI coding agents could eventually work fully autonomously, I don’t think they’ll replace humans entirely because there will still be people who want to get things done, and new AI power tools will emerge to help them do it.&lt;/p&gt;
&lt;p&gt;AI coding tools can turn what was once a year-long personal project into a five-minute session. I fed Claude Code a photo of a two-player Tetris game I sketched in a notebook back in 2008, and it produced aworking prototypein minutes (prompt: “create a fully-featured web game with sound effects based on this diagram”). That’s wild, and even though the results are imperfect, it’s a bit frightening to comprehend what kind of sea change in software development this might entail.&lt;/p&gt;
&lt;p&gt;Since early December, I’ve beenpostingsome of my more amusing experimental AI-coded projects to Bluesky for people to try out, but I discovered I needed to deliberately slow down with updates because they came too fast for people to absorb (and too fast for me to fully test). I’ve also received comments like “I’m worried you’re using AI, you’re making games too fast” and so on.&lt;/p&gt;
&lt;p&gt;Regardless of my own habits, the flow of new software will not slow down. There will soon be a seemingly endless supply of AI-augmented media (games, movies, images, books), and that’s a problem we’ll have to figure out how to deal with. These products won’t all be “AI slop,” either; some will be done very well, and the acceleration in production times due to these new power tools will balloon the quantity beyond anything we’ve seen.&lt;/p&gt;
&lt;p&gt;Social media tends to prime people to believe that AI is all good or all bad, but that kind of black-and-white thinking may be the easy way out. You’ll have no cognitive dissonance, but you’ll miss a far richer third option: seeing these tools as imperfect and deserving of critique but also as useful and empowering when they bring your ideas to life.&lt;/p&gt;
&lt;p&gt;AI agents should be considered tools, not entities or employees, and they should be amplifiers of human ideas. My game-in-progressCard Mineris entirely my own high-level creative design work, but the AI model handled the low-level code. I am still proud of it as an expression of my personal ideas, and it would not exist without AI coding agents.&lt;/p&gt;
&lt;p&gt;For now, at least, coding agents remain very much tools in the hands of people who want to build things. The question is whether humans will learn to wield these new tools effectively to empower themselves. Based on two months of intensive experimentation, I’d say the answer is a qualified yes, with plenty of caveats.&lt;/p&gt;
&lt;p&gt;We also have social issues to face: Professional developers already use these tools, and with the prevailing stigma against AI tools in some online communities, many software developers and the platforms that host their work will face difficult decisions.&lt;/p&gt;
&lt;p&gt;Ultimately, I don’t think AI tools will make human software designers obsolete. Instead, they may well help those designers become more capable. This isn’t new, of course; tools of every kind have been serving this role since long before the dawn of recorded history. The best tools amplify human capability while keeping a person behind the wheel. The 3D printer analogy holds: amazing fast results are possible, but mastery still takes time, skill, and a lot of patience with the machine.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/</guid><pubDate>Mon, 19 Jan 2026 03:00:45 +0000</pubDate></item><item><title>Meet Veronika, the tool-using cow</title><link>https://arstechnica.com/science/2026/01/meet-veronika-the-tool-using-cow/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/cowtool1-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Veronika uses sticks to scratch herself, suggesting scientists have underestimated cow cognition&lt;/p&gt;
&lt;p&gt;Far Sidefans might recall a classic 1982 cartoon called “Cow Tools,” featuring a cow standing next to a jumble of strange objects—the joke being that cows don’t use tools. That’s why a pet Swiss brown cow in Austria named Veronika has caused a bit of a sensation: she likes to pick up random sticks and use them to scratch herself. According to anew paperpublished in the journal Current Biology, this is a form of multipurpose tool use and suggests that the cognitive capabilities of cows have been underestimated by scientists.&lt;/p&gt;
&lt;p&gt;Aspreviously reported, tool use was once thought to be one of the defining features of humans, but examples of it were eventually observed in primates and other mammals. Dolphins can toss objects as a form of play which some scientists consider to be a type of tool use, particularly when it involves another member of the same species. Potential purposes include a means of communication, social bonding, or aggressiveness. (Octopuses have alsobeen observedengaging in similar throwing behavior.)&lt;/p&gt;
&lt;p&gt;But the biggest surprise came when birds were observed using tools in the wild. After all, birds are the only surviving dinosaurs, and mammals and dinosaurs hadn’t shared a common ancestor for hundreds of millions of years. In the wild, observedtool use has been limitedto the corvids (crows and jays), which show a variety of other complex behaviors—they’llremember your faceand recognize thepassing of their dead.&lt;/p&gt;
&lt;p&gt;In 2012, a captive cockatoo named Figaro was trying to retrieve a stone that had fallen behind a metal divider and picked up a stick to increase his reach. He failed but then the researchers placed a nut behind the divider as an added incentive. Figaro initially picked up a stick from the enclosure’s floor, but this proved to be too short to reach the food. So, he actually splintered off a piece of the enclosure’s wooden base, and successfully used that to pull the nut towards the wire until he could use his beak to grab it.&lt;/p&gt;
&lt;p&gt;Figaro used a different tool in each subsequent trial, and in most cases made some modifications to it before successfully retrieving a nut. In at least one case, he performed four separate modifications before putting a stick to use in retrieving the nut. He also managed to use the tool in two different ways, often alternating dragging and sweeping motions in his efforts to pull the food within reach. Other cockatoos who repeatedly watched Figaro’s performance were also able to do so. By 2022, Figaro and his cockatoo cronieshad learnedhow tocombine tools—in this case, a stick and a ball—to play a rudimentary form of “golf.”&lt;/p&gt;
&lt;p&gt;For all the scientific interest in animal tool use, there has been almost no research into the cognitive capabilities of livestock like cows, or their possible tool use, according to the authors of this latest paper. Alice Auersperg, a cognitive biologist at the University of Veterinary Medicine in Vienna, was intrigued when she watched the footage of Veronika’s scratching behavior. “It was clear that this was not accidental,”said Auersperg. “This was a meaningful example of tool use.”&lt;/p&gt;
&lt;p&gt;To test the extent of Veronika’s tool use, Auersperg and her postdoc, Antonio Osuna-Mascaro, visited the farm where Veronika lives. Her owner, Witgar Wiegele, is an organic farmer and baker who keeps the cow as a pet. With Wiegele’s permission, they conducted a series of randomized trials with a deck scrub broom, chosen for its asymmetrical shape, placed in different orientations. The pair recorded 76 instances of Veronika using the broom over seven sessions of ten trials.&lt;/p&gt;
&lt;p&gt;Each time, Veronika used her tongue to lift and position the broom in her mouth, clamping down with her teeth for a stable grip. This enabled her to use the broom to scratch otherwise hard-to-reach areas on the rear half of her body. Veronika seemed to prefer the brush end to the stick end (i.e., the exploitation of distinct properties of a single object for different functions) although which end she used depended on body area. For example, she used the brush end to scratch her upper body using a scrubbing motion, while using the stick end to scratch more sensitive lower areas like her udders and belly skin flaps using precisely targeted gentle forward pushes. She also anticipated the need to adjust her grip.&lt;/p&gt;
&lt;p&gt;The authors conclude that this behavior demonstrates “goal-directed, context-sensitive tooling,” as well as versatility in her tool-use anticipation, and fine-motor targeting. Veronika’s scratching behavior is likely motivated by the desire to relieve itching from insect bites, but her open, complex environment, compared to most livestock, and regular interactions with humans enabled her unusual cognitive abilities to emerge.&lt;/p&gt;
&lt;p&gt;The implication is that this kind of technical problem-solving is not confined to species with large brains and hands or beaks. “[Veronika] did not fashion tools like the cow in Gary Larson’s cartoon, but she selected, adjusted, and used one with notable dexterity and flexibility,” the authors wrote. “Perhaps the real absurdity lies not in imagining a tool-using cow, but in assuming such a thing could never exist.”&lt;/p&gt;
&lt;p&gt;DOI: Current Biology, 2025.10.1016/j.cub.2025.11.059(About DOIs).&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/science/2026/01/meet-veronika-the-tool-using-cow/</guid><pubDate>Mon, 19 Jan 2026 07:00:27 +0000</pubDate></item><item><title>The race to build a super-large ground telescope is likely down to two competitors</title><link>https://arstechnica.com/space/2026/01/as-europes-large-ground-telescope-project-advances-how-is-its-us-competitor-faring/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GMT-Rendering-IDOM-Enclosure-2024-1-scaled-1-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Ars checks in with the new president of the Giant Magellan Telescope.&lt;/p&gt;
&lt;p&gt;I have been writing about the Giant Magellan Telescope for a long time.Nearly two decades ago, for example, I wrote that time was “running out” in the race to build the next great optical telescope on the ground.&lt;/p&gt;
&lt;p&gt;At the time the proposed telescope was one of three contenders to make a giant leap in mirror size from the roughly 10-meter diameter instruments that existed then, to approximately 30 meters. This represented a huge increase in light-gathering potential, allowing astronomers to see much further into the universe—and therefore back into time—with far greater clarity.&lt;/p&gt;
&lt;p&gt;Since then the projects have advanced at various rates. An international consortium to build the Thirty Meter Telescope in Hawaii ran into local protests that have bogged down development. Its future came further into question when the US National Science Foundationdropped supportfor the project in favor of the Giant Magellan Telescope. Meanwhile the European Extremely Large Telescope (ELT) has advanced on a faster schedule, and this 39.5-meter telescope could observe its first light in 2029.&lt;/p&gt;
&lt;p&gt;This leaves the Magellan telescope. Originally backers of the GMT intended it to be fully operational by now, but it has faced funding and technology challenges. It has a price tag of approximately $2 billion, and although it is smaller than the European project, the 25.4-meter telescope now represents the best avenue for US-based astronomy to remain competitive in the field.&lt;/p&gt;
&lt;p&gt;Given all of this, I recently spoke with University of Texas at Austin astronomer Dan Jaffe, who is the new president of the telescope’s executive team, to get an update on things. Here is a lightly edited transcript of our conversation.&lt;/p&gt;
&lt;p&gt;Ars Technica: What should we know about the Giant Magellan Telescope?&lt;/p&gt;
&lt;p&gt;Dan Jaffe: This is going to be one of the premier next-generation optical infrared telescopes in the world. It will give the United States astronomical community access that helps us to be a leading nation in this field, inspire students to go into science and engineering, and really enrich the human experience through the new knowledge that we get about the nature of the universe. So I think it covers both this kind of aspiration that we have to enrich humanity in some way, to help foster the future economy by bringing more people into these technical fields, and also by driving technology in some areas. The kinds of work we’re doing on adaptive optics, for example, in building sensitive detector systems and spectrometers, drive the frontier of what you can do with these systems.&lt;/p&gt;
&lt;p&gt;Ars: Why has this taken so long?&lt;/p&gt;
&lt;p&gt;Jaffe: Well, it’s large, it’s complicated, and it’s unique. Those are the three things together that drive that. It involves a large amount of technology that’s needed to make something like this go in terms of size and precision and speed. And those things together make it take a long time, and also cost a fairly large amount of money. We’ve raised about half of the money. And many of the most difficult parts of the project are well underway in terms of construction or prototyping. All seven mirrors that are needed for the telescope are cast, and a number of them are already finished.&lt;/p&gt;
&lt;p&gt;Ars: How is the telescope’s site in the Atacama Desert in Chile coming along?&lt;/p&gt;
&lt;p&gt;Jaffe: We’ve broken ground on the site, and it’s been leveled in the foundation. Areas have been dug out, and the utilities have all been installed. It’s now kind of in a free state, waiting for some of the other parts of the project to catch up to it.&lt;/p&gt;
&lt;p&gt;Ars: Since the European telescope is now likely to be completed first, does that mean GMT will miss out on a lot of science discoveries?&lt;/p&gt;
&lt;p&gt;Jaffe: These telescopes are complementary in many ways. It depends not just on the glass itself, but on the instruments you put behind them, and what they’re capable of. And I expect that at this size scale there’ll be plenty of discoveries for all of us to make. And also with other things coming online, like the (Vera) Rubin telescope that will provide new types of phenomena to investigate, there won’t be a list of things we’ve got to do, and the first person to get to the finish line will do them. I think there’s going to be exciting science for all of us. Our community is growing right now. MIT and Northwestern have joined our consortium in the last year-and-a-half or two years, and I think we’re going to have plenty of very exciting science to do. As a public-private partnership for the whole US community, there’s a tremendous amount of brain power out there to be creative about new things and to be innovative, both in the kind of instrumentation we build and in the ways we use it.&lt;/p&gt;
&lt;p&gt;Ars: How concerned are you about megaconstellations impacting observations by GMT?&lt;/p&gt;
&lt;p&gt;Jaffe: Astronomers have always had to manage sources of noise including satellites, high-energy particles detected by instruments, and electronic noise. We address these with a variety of well-tested observing strategies and data-processing techniques that generally mitigate such issues very effectively. Satellites are most problematic for telescopes that are specifically designed to do surveys that cover wide areas of the sky and focus on imaging, not spectroscopy. The data and discoveries from these projects become the input and starting points for research on larger telescopes. The Giant Magellan Telescope will focus on spectroscopy of objects over small fields of view compared to a survey telescope—even when those targets are spread across wide areas of the sky. As a result, we are far less affected than large survey telescopes. We are, of course, always working to further improve our observing and data-processing strategies. At the same time, the astronomical community is working collaboratively with satellite operators to reduce their impact on science by altering satellite brightness and altitude and improving prediction and tracking. We’re confident that continued cooperation will allow ground-based astronomy to advance and coexist successfully with satellite technologies.&lt;/p&gt;
&lt;p&gt;Ars: What are you most excited to observe with the telescope?&lt;/p&gt;
&lt;p&gt;Jaffe: My own interest is in studying planetary systems, how they evolve, and how they form planets. And there I would say the giant telescopes, in particular the GMT because the implementation it has, will help to move us from a discovery phase to an investigation phase, where we try to learn about these planets around other stars. What are they made of? How hot are they? How are their atmospheres behaving? Things like that. So the capabilities of this telescope will put us into an era in the exoplanet world of really starting to learn about the nature of these exoplanets, not just finding more.&lt;/p&gt;
&lt;p&gt;Ars: What kinds of things could we learn?&lt;/p&gt;
&lt;p&gt;Jaffe: Well, in particular with my instrument, which breaks up the light into very fine intervals, you can study which molecules are present in exoplanet atmospheres much more precisely, and the ratio of the different isotopes, which tells you about where those molecules came from. You can measure temperatures. You can look at wind velocities. There are many, many things you can do that help you to study these planets in much more detail.&lt;/p&gt;
&lt;p&gt;Ars: We’ve got this new generation of ground-based observatories coming online. What could we be doing with new space telescopes to complement this?&lt;/p&gt;
&lt;p&gt;Jaffe: Generically, I would say there are two things you can do in space that are particularly helpful for ground-based astronomy. You can look at wavelengths that you can’t see from the ground. So ultraviolet and X-rays, in particular, are very important in that respect. The other thing is that you can look at things continuously, if you are in certain types of orbits. And so people are building a number of very small, specialized telescopes to just look at things for a long time. The Kepler observatory is a great example of this, for looking for those little dips as planets move in front of their host stars, something that people do from the ground, but it’s much, much harder. So depending upon the field, there are various complementary space projects of different scale that could be important. And you know, the GMT will follow-up on some of the discoveries with the James Webb Space Telescope. We have higher spatial resolution, so we’re able to look in more detail at some of the objects that were looked at there. We have different types of instrumentation that were not thought of when JWST was developed that can help with things. And I think the reverse will happen, as you were alluding to, that as we start to move into this era with the Giant Magellan Telescope, that this will engender certain types of space experiments that are that are targeted to the kinds of questions that we raise.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/space/2026/01/as-europes-large-ground-telescope-project-advances-how-is-its-us-competitor-faring/</guid><pubDate>Mon, 19 Jan 2026 08:06:28 +0000</pubDate></item><item><title>Reports of ad-supported Xbox game streams show Microsoft’s lack of imagination</title><link>https://arstechnica.com/gaming/2026/01/reports-of-ad-supported-xbox-game-streams-show-microsofts-lack-of-imagination/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2021/06/Cloud-Gaming_iPadSurfaceiPhone-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Xbox maker needs some fresher ideas for expanding access to cloud gaming.&lt;/p&gt;
&lt;p&gt;Currently, Microsoft’slong-runningCloud Gaming service is limited to players that have aMicrosoft’s Game Pass subscription. Now, new reporting suggests Microsoft is planning to offer non-subscribers access to game streams paid for by advertising in the near future, but only in extremely limited circumstances.&lt;/p&gt;
&lt;p&gt;The latest wave of rumors was set off late last week when The Verge’s Tom Warrenshared an Xbox Cloud Gaming loading screenwith a message mentioning “1 hour of ad supported playtime per session.” That leaked message comes afterWindows Central reported last summerthat Microsoft has been “exploring video ads for free games for quite some time,” à lathe two-minute sponsorships that appear before free-tier game streams on Nvidia’s GeForce Now service.&lt;/p&gt;
&lt;p&gt;Don’t get your hopes up for easy, free, ad-supported access to the entire Xbox Cloud Gaming library, though. Windows Centralnow reportsthat Microsoft will be using ads merely to slightly expand access toits “Stream your own game” program. That program currently offers subscribers to the Xbox Game Pass Essentials tier (or higher) the privilege of streaming versions of some of the Xbox games they’ve already purchased digitally. Windows Central’s unnamed sources suggest a “session-based ad-supported access tier” to stream those purchased games will be offered to non-subscribers as soon as “this year.”&lt;/p&gt;
&lt;p&gt;That’s a moderately useful option for cloud-curious Xbox players that might not be willing to take the plunge on a monthly subscription, we suppose. But it also feels like Microsoft could come up with some more imaginative ways to use Cloud Gaming to reach occasional players in new ways.&lt;/p&gt;
&lt;p&gt;What’s stopping Microsoft from offer streaming players a 30-minute timed demo stream ofanyavailable Xbox Cloud Gaming title—perhaps in exchange for watching a short ad, or perhaps simply asan Xbox Live Arcade-style sales juicing tactic? Or why not offer discounted access to a streaming-only Game Pass subscription for players willing to watch occasional ads,like Netflix? Microsoft could even let players spend a couple of bucks to rent a digital copy of the title for a few days, much as services like iTunes do for newer films.&lt;/p&gt;
&lt;p&gt;Those are just a few ideas off the top of our heads. And they all feel potentially more impactful than using ads as a way to let Xbox players stream copies of games they already purchased.&lt;/p&gt;
&lt;p&gt;Back in 2019, wenotedhow Stadia’s strictly buy-before-you-play streaming business model limited the appeal of what ended up asa doomed cloud-gaming experiment. Microsoft should take some lessons from Google’s failure and experiment with new ways to use streaming to reach players that might not have access to the latest high-end hardware for their gaming experiences.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/gaming/2026/01/reports-of-ad-supported-xbox-game-streams-show-microsofts-lack-of-imagination/</guid><pubDate>Mon, 19 Jan 2026 08:16:38 +0000</pubDate></item><item><title>Asus confirms its smartphone business is on indefinite hiatus</title><link>https://arstechnica.com/gadgets/2026/01/asus-confirms-its-smartphone-business-is-on-indefinite-hiatus/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/ROG-2-1-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Asus chairman Jonney Shih sees AI applications as the company’s main focus going forward.&lt;/p&gt;
&lt;p&gt;An unconfirmed report early this month suggested Asus was pulling back on its smartphone plans, but the company declined to comment at the time. Asus chairman Jonney Shih has now confirmed the wind-down of its smartphone business during an event in Taiwan. Instead, Asus will focus on AI products like robots and smart glasses.&lt;/p&gt;
&lt;p&gt;Shih addressed the company’s future plans during a 2026 kick-off event in Taiwan, as reported byInside. “Asus will no longer add new mobile phone models in the future,” said Shih (machine translated).&lt;/p&gt;
&lt;p&gt;So don’t expect a new Zenfone or ROG Phone from Asus in 2026. That said, very few phone buyers were keeping tabs on the latest Asus phones anyway, which is probably why Asus is throwing in the towel. Shih isn’t saying Asus won’t ever release a new phone, but the company will take an “indefinite wait-and-see” approach. Again, this is a translation and could be interpreted in multiple ways.&lt;/p&gt;
&lt;p&gt;The Zenfone line might not be missed—its claim to fame was being slightly smaller and cheaper than competing devices, but Asus’ support and update policy were lightyears behind the market leaders. TheROG Phone linehas been prominent in the gaming phone niche, offering the latest chipsets with active cooling, multiple USB-C ports, game controller accessories, blinking lights, and even a headphone jack. However, ROG Phones are even more expensive than Samsung’s flagship devices, with the most recent ROG Phone 9 Pro starting at $1,200. Apparently, the market of those who aren’t happy gaming on the latest iPhone or Samsung Galaxy is miniscule.&lt;/p&gt;
&lt;p&gt;Existing Asus devices should continue to get updates, but Asus never took the lead there. The lavishly expensive ROG Phone 9 Pro is only guaranteed two OS updates and five years of security patches. The most recent Zenfones are also only eligible for two Android version updates, but they get just four years of security support.&lt;/p&gt;
&lt;p&gt;Shih’s comments imply that Asus won’t get back into the phone game unless something changes, and that’s not likely. Asus is not the first OEM to drop phone plans, and this is a continuation of a trend that has been underway for years as people upgrade phones less often.&lt;/p&gt;
&lt;p&gt;Asus used to be a major player in the Android device ecosystem, offering myriad phones, tablets, phones that turned into tablets, andtablets that turned into PCs(sort of). The rapidly expanding market and advancing technology in the late 2000s and early 2010s left room for companies like Asus to operate alongside bigger OEMs like Samsung and Apple. Those were the days when every smartphone preference was served, from keyboarded sliders, to devices with integrated projectors, to the flat slab phones that eventually won out.&lt;/p&gt;
&lt;p&gt;Today, smartphones have become a mature technology, and there simply isn’t as much room for improvement year-to-year. Combine that with rising prices, and people are apt to keep their devices longer. The continued rise of Chinese OEMs like Vivo, Xiaomi, and Huawei also makes it more difficult for niche players—particularly those focused on markets outside the US—to earn money designing and manufacturing a new smartphone every year. And as soon as you stop doing that, other brands are faster and more well-supported when the time does come to pick up a new phone.&lt;/p&gt;
&lt;p&gt;So far, no Android device maker that has taken a break from releasing phones has ever ramped back up. Just ask LG, which once traded blows with hometown rival Samsung in smartphones. LG’s mobile division lost money for years, leading it to scale back its release schedule in 2019. At the time, the company was adamant it would release new phones when it had a good reason. A few years later, LG’s mobile divisioncalled it quits.&lt;/p&gt;
&lt;p&gt;The possible end of Asus phones further narrows the market, leaving phone buyers with fewer choices. That doesn’t matter to Asus, though, which is a company that exists to make money. While announcing an indefinite pause in smartphone releases, Shih also noted that the company saw a 26.1 percent increase in revenue for 2025, thanks in large part to a doubling of its AI server business. So Asus is focusing on that part of the market. Maybe that’s not the best long-term strategy, but neither is losing money on smartphones.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/gadgets/2026/01/asus-confirms-its-smartphone-business-is-on-indefinite-hiatus/</guid><pubDate>Mon, 19 Jan 2026 09:24:49 +0000</pubDate></item><item><title>Elon Musk accused of making up math to squeeze $134B from OpenAI, Microsoft</title><link>https://arstechnica.com/tech-policy/2026/01/elon-musk-accused-of-making-up-math-to-squeeze-134b-from-openai-microsoft/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2087343447-1024x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Musk’s math reduced ChatGPT inventors’ contributions to “zero,” OpenAI argued.&lt;/p&gt;
&lt;p&gt;Elon Musk is going for some substantial damages in his lawsuit accusing OpenAI of abandoning its nonprofit mission and “making a fool out of him” as an early investor.&lt;/p&gt;
&lt;p&gt;On Friday, Musk filed anoticeon remedies sought in the lawsuit, confirming that he’s seeking damages between $79 billion and $134 billion from OpenAI and its largest backer, co-defendant Microsoft.&lt;/p&gt;
&lt;p&gt;Musk hired an expert he has never used before, C. Paul Wazzan, who reached this estimate by concluding that Musk’s early contributions to OpenAI generated 50 to 75 percent of the nonprofit’s current value. He got there by analyzing four factors: Musk’s total financial contributions before he left OpenAI in 2018, Musk’s proposed equity stake in OpenAI in 2017, Musk’s current equity stake in xAI, and Musk’s nonmonetary contributions to OpenAI (like investing time or lending his reputation).&lt;/p&gt;
&lt;p&gt;The eye-popping damage claim shocked OpenAI and Microsoft, which could also face punitive damages in a loss.&lt;/p&gt;
&lt;p&gt;The tech giants immediately filed amotionto exclude Wazzan’s opinions, alleging that step was necessary to avoid prejudicing a jury. Their filing claimed that Wazzan’s math seemed “made up,” based on calculations the economics expert testified he’d never used before and allegedly “conjured” just to satisfy Musk.&lt;/p&gt;
&lt;p&gt;For example, Wazzan allegedly ignored that Musk left OpenAI after leadership did not agree on how to value Musk’s contributions to the nonprofit. Problematically, Wazzan’s math depends on an imaginary timeline where OpenAI agreed to Musk’s 2017 bid to control 51.2 percent of a new for-profit entity that was then being considered. But that never happened, so it’s unclear why Musk would be owed damages based on a deal that was never struck, OpenAI argues.&lt;/p&gt;
&lt;p&gt;It’s also unclear why Musk’s stake in xAI is relevant, since OpenAI is a completely different company not bound to match xAI’s offerings. Wazzan allegedly wasn’t even given access to xAI’s actual numbers to help him with his estimate, only referring to public reporting estimating that Musk owns 53 percent of xAI’s equity. OpenAI accused Wazzan of including the xAI numbers to inflate the total damages to please Musk.&lt;/p&gt;
&lt;p&gt;“By all appearances, what Wazzan has done is cherry-pick convenient factors that correspond roughly to the size of the ‘economic interest’ Musk wants to claim, and declare that those factors support Musk’s claim,” OpenAI’s filing said.&lt;/p&gt;
&lt;p&gt;Further frustrating OpenAI and Microsoft, Wazzan opined that Musk and xAI should receive the exact same total damages whether they succeed on just one or all of the four claims raised in the lawsuit.&lt;/p&gt;
&lt;p&gt;OpenAI and Microsoft are hoping the court will agree that Wazzan’s math is an “unreliable… black box” and exclude his opinions as improperly reliant on calculations that cannot be independently tested.&lt;/p&gt;
&lt;p&gt;Microsoft could not be reached for comment, but OpenAI has alleged thatMusk’s suit is a harassment campaignaimed at stalling a competitor so that his rival AI firm, xAI, can catch up.&lt;/p&gt;
&lt;p&gt;“Musk’s lawsuit continues to be baseless and a part of his ongoing pattern of harassment, and we look forward to demonstrating this at trial,” an OpenAI spokesperson said in a statement provided to Ars. “This latest unserious demand is aimed solely at furthering this harassment campaign. We remain focused on empowering the OpenAI Foundation, which is already one of the best resourced nonprofits ever.”&lt;/p&gt;
&lt;p&gt;Wazzan is “a financial economist with decades of professional and academic experience who has managed his own successful venture capital firm that provided seed-level funding to technology startups,” Musk’s filing said.&lt;/p&gt;
&lt;p&gt;OpenAI explained how Musk got connected with Wazzan, who testified that he had never been hired by any of Musk’s companies before. Instead, three months before he submitted his opinions, Wazzan said that Musk’s legal team had reached out to his consulting firm, BRG, and the call was routed to him.&lt;/p&gt;
&lt;p&gt;Wazzan’s task was to figure out how much Musk should be owed after investing $38 million in OpenAI—roughly 60 percent of its seed funding. Musk also made nonmonetary contributions Wazzan had to weigh, like “recruiting key employees, introducing business contacts, teaching his cofounders everything he knew about running a successful startup, and lending his prestige and reputation to the venture,” Musk’s filing said.&lt;/p&gt;
&lt;p&gt;The “fact pattern” was “pretty unique,” Wazzan testified, while admitting that his calculations weren’t something you’d find “in a textbook.”&lt;/p&gt;
&lt;p&gt;Additionally, Wazzan had to factor in Microsoft’s alleged wrongful gains, by deducing how much of Microsoft’s profits went back into funding the nonprofit. Microsoft alleged Wazzan got this estimate wrong after assuming that “some portion of Microsoft’s stake in the OpenAI for-profit entity should flow back to the OpenAI nonprofit” and arbitrarily decided that the portion must be “equal” to “the nonprofit’s stake in the for-profit entity.” With this odd math, Wazzan double-counted value of the nonprofit and inflated Musk’s damages estimate, Microsoft alleged.&lt;/p&gt;
&lt;p&gt;“Wazzan offers no rationale—contractual, governance, economic, or otherwise—for reallocating any portion of Microsoft’s negotiated interest to the nonprofit,” OpenAI’s and Microsoft’s filing said.&lt;/p&gt;
&lt;p&gt;Perhaps most glaringly, Wazzan reached his opinions without ever weighing the contributions of anyone but Musk, OpenAI alleged. That means that Wazzan’s analysis did not just discount efforts of co-founders and investors like Microsoft, which “invested billions of dollars into OpenAI’s for-profit affiliate in the years after Musk quit.” It also dismissed scientists and programmers who invented ChatGPT as having “contributed zero percent of the nonprofit’s current value,” OpenAI alleged.&lt;/p&gt;
&lt;p&gt;“I don’t need to know all the other people,” Wazzan testified.&lt;/p&gt;
&lt;p&gt;Wazzan supposedly also did not bother to quantify Musk’s nonmonetary contributions, which could be in the thousands, millions, or billions based on his vague math, OpenAI argued.&lt;/p&gt;
&lt;p&gt;Even Musk’s legal team seemed to contradict Wazzan, OpenAI’s filing noted. In Musk’s filing on remedies, it’s acknowledged that the jury may have to adjust the total damages. Because Wazzan does not break down damages by claims and merely assigns the same damages to each individual claim, OpenAI argued it will be impossible for a jury to adjust any of Wazzan’s black box calculations.&lt;/p&gt;
&lt;p&gt;“Wazzan’s methodology is made up; his results unverifiable; his approach admittedly unprecedented; and his proposed outcome—the transfer of billions of dollars from a nonprofit corporation to a donor-turned competitor—implausible on its face,” OpenAI argued.&lt;/p&gt;
&lt;p&gt;At a trial starting in April, Musk will strive to convince a court that such extraordinary damages are owed. OpenAI hopes he’ll fail, in part since “it is legally impossible for private individuals to hold economic interests in nonprofits” and “Wazzan conceded at deposition that he had no reason to believe Musk ‘expected a financial return when he donated… to OpenAI nonprofit.’”&lt;/p&gt;
&lt;p&gt;“Allowing a jury to hear a disgorgement number—particularly one that is untethered to specific alleged wrongful conduct and results in Musk being paid amounts thousands of times greater than his actual donations—risks misleading the jury as to what relief is recoverable and renders the challenged opinions inadmissible,” OpenAI’s filing said.&lt;/p&gt;
&lt;p&gt;Wazzan declined to comment. xAI did not immediately respond to Ars’ request to comment.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/01/elon-musk-accused-of-making-up-math-to-squeeze-134b-from-openai-microsoft/</guid><pubDate>Mon, 19 Jan 2026 10:04:18 +0000</pubDate></item><item><title>Signs point to a sooner-rather-than-later M5 MacBook Pro refresh</title><link>https://arstechnica.com/gadgets/2026/01/signs-point-to-a-sooner-rather-than-later-m5-macbook-pro-refresh/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_6215-1152x648.jpeg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Delayed shipping times for current models sometimes means an update is imminent.&lt;/p&gt;
&lt;p&gt;Mac power users waiting on new high-end MacBook Pro models may have been disappointed last fall, when Applereleased an M5 upgrade for the low-end 14-inch MacBook Prowithout touching the M4 Pro or Max versions of the laptop. But the wait for M5 Pro and M5 Max models may be nearing its end.&lt;/p&gt;
&lt;p&gt;The tea-leaf readers at MacRumorsnoticedthat shipping times for a handful of high-end MacBook Pro configurations have slipped into mid-to-late February, rather than being available immediately as most Mac models are. This is often, though not always, a sign that Apple has slowed down or stopped production of an existing product in anticipation of an update.&lt;/p&gt;
&lt;p&gt;Currently, the shipping delays affect the M4 Max versions of both the 14-inch and 16-inch MacBook Pros. If you order them today, these models will arrive sometime between February 3 and February 24, depending on the configuration you choose; many M4 Pro versions are still available for same-day shipping, though adding a nano-texture display or upgrading RAM can still add a week or so to the shipping time.&lt;/p&gt;
&lt;p&gt;Apple could choose to launch new Pro hardware on January 28, to go with the newCreator Studio subscriptionit announced last week. Aimed primarily at independent content creators that make their own video, audio, and images, the Creator Studio subscription bundles Final Cut Pro, Logic Pro, Pixelmator Pro, and enhancements for the Pages, Numbers, and Keynote apps (along with some other odds and ends) for $13 a month or $130 a year. None of these apps require a MacBook Pro, but many would benefit in some way from the additional CPU and GPU power, RAM, and storage available in Apple’s high-end laptops.&lt;/p&gt;
&lt;p&gt;Of course, an imminent replacement isn’t the only reason why the shipping estimates for any given Mac might slip.Ongoing, AI-fueled RAM shortagescould be causing problems, and Apple probably prioritizes production of the widely-used base-model M4 and M5 chips to the larger, more expensive, more complex Max models.&lt;/p&gt;
&lt;p&gt;But the only other device in Apple’s lineup that offers the M4 Max and similar RAM configuration options is the high-end Mac Studio, which currently isn’t subject to the same shipping delays. That does imply that the delays are specific to the MacBook Pro—and one explanation for this is that the laptop is about to be replaced.&lt;/p&gt;
&lt;p&gt;Virtually every Mac in Apple’s lineup is ready for an M5-series refresh. Only the low-end 14-inch MacBook Pro got an M5 upgrade last year, a relatively restrained rollout compared to the M3 and M4 generations—the M4 generation hit all the MacBook Pros, the iMac, and the Mac mini all at the same time. That’s left a lot of Macs eligible for an update at some point in 2026, including the Pros, the MacBook Airs, the Mac mini, the iMac, the Mac Studio, and the Mac Pro tower.&lt;/p&gt;
&lt;p&gt;Apple has been known to skip Apple Silicon chip generations for some models in the past, so a handful of systems may stick with the M4 until the M6 rolls around. The MacBook Pros and Airs are reliably updated with each new chip generation, but the desktop Macs have all missed at least one chip update since the Apple Silicon era began.&lt;/p&gt;
&lt;p&gt;Reporting from Bloomberg’s Mark Gurman alsosuggeststhat the Mac Pro may be on life support—even if an M5 Ultra chip arrives for the Mac Studio, there’s no guarantee that the Mac Pro (currently still rocking an M2 Ultra) will get the same chip.&lt;/p&gt;
&lt;p&gt;On top of the existing Macs that we expect to see updated, Apple is rumored to be introducing an all-new Mac this year:a new entry-level modelwith an iPhone-class Apple A-series processor inside. This device is said to be a direct replacement for the M1 MacBook Air that Applestill sells for $599 via Walmartin the US, years after removing the model from its own retail and online stores.&lt;/p&gt;
&lt;p&gt;Expect any replacement to share a lot in common with the old M1 Air, including its 13-inch screen size and (possibly) its paltry-by-modern-standards 8GB of RAM. But for $400 less than the starting price of the M4 MacBook Air, this low-end Mac would still serve a kind of customer that would otherwise have only Windows laptops and Chromebooks to consider.&lt;/p&gt;
&lt;p&gt;The Walmart website currently has “low stock” of the M1 MacBook Air—it may be that we can also expect that new low-end MacBook sooner rather than later.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/gadgets/2026/01/signs-point-to-a-sooner-rather-than-later-m5-macbook-pro-refresh/</guid><pubDate>Mon, 19 Jan 2026 10:52:22 +0000</pubDate></item><item><title>The first new Marathon game in decades will launch on March 5</title><link>https://arstechnica.com/gaming/2026/01/bungies-delayed-marathon-revival-will-finally-launch-march-5/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/marathon-1152x648.png" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Development hasn’t exactly been smooth since the extraction shooter’s 2023 announcement.&lt;/p&gt;
&lt;p&gt;It’s been nearly three years now sinceDestinymaker (andSony subsidiary) Bungieformally announceda revival of the storiedMarathonFPS franchise. And it has been about seven months since the game’s original announced release date of September 23, 2025 waspushed back indefinitelyafter a reportedly poor response to the game’s first Alpha test.&lt;/p&gt;
&lt;p&gt;But today, ina post on the PlayStation Blog, Bungie revealed that the newMarathonwould finally be hitting PS5, Windows, and Xbox Series X|S on March 5, narrowing down the month-long March release windowannounced back in December.&lt;/p&gt;
&lt;p&gt;UnlikeDestiny 2, whichtransitioned to a free-to-play model in 2019, the newMarathonsells for $40 in a Standard Edition or a $60 Deluxe Edition that includes some digital rewards and cosmetics. That mirrors the pricing of the somewhat similarArc Raiders, which recently hit 12 million sales in less than 12 weeks.&lt;/p&gt;
&lt;p&gt;Unlike the originalMarathontrilogy on the ’90s Macintosh—which closely followed on the single-player campaign corridors and deathmatch multiplayer of the originalDoom—the newMarathonis described as a “PvPvE survival extraction shooter.” That means gameplay based around exploring distinct zones and scavenging for cosmetics and gear upgrades in exploratory missions alone or with up to two friends, then seeing those missions “break into fast-paced PvP combat” at a moment’s notice, according to the game’s official description.&lt;/p&gt;
&lt;p&gt;Anew preorder trailerexpands onother more gameplay-focused videosshowcasing the poisoned world of Tau Ceti IV and the six competing factions of highly customizable “runner shells” they’ll inhabit. Bungie has also revealed a scavenging-focused “Rook” option for players that want to queue up solo and join a match in progress with nothing but a starting kit.&lt;/p&gt;
&lt;p&gt;“Instead of focusing on progression and questing and things of that sort, [Rook] is a mode where we make it easy for you to just kind of drop into a match that’s in progress and run about and try to scavenge as much as possible and get out with it, so that you can populate your vault a little bit more for future runs,” Game Director Joe Ziegler saidin a recent video.&lt;/p&gt;
&lt;p&gt;TheMarathonrevival project has faced its fair share of development troubles in recent years. That includespublic accusations that early test gameplay included plagiarized artwork, leading the development team toannouncea “thorough review of our in-game assets.” Internal plans to release the game in 2024 werereportedly pushed back and entire yearamid pessimism among developers. More recently, the gamegot a new creative director in June, which isn’t exactly a sign of confidence that late in a long-running project.&lt;/p&gt;
&lt;p&gt;After all that—and given a Bungie gaming pedigree that dates back to the originalMarathon—it’s fair to say that the newMarathonis going to get more than its fair share of attention when it finally gets into players’ hands in just six weeks.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/gaming/2026/01/bungies-delayed-marathon-revival-will-finally-launch-march-5/</guid><pubDate>Mon, 19 Jan 2026 12:07:55 +0000</pubDate></item><item><title>The fastest human spaceflight mission in history crawls closer to liftoff</title><link>https://arstechnica.com/space/2026/01/nasas-artemis-ii-rocket-rolls-to-launch-pad-but-key-test-looms-ahead/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_0127-1-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;After a remarkably smooth launch campaign, Artemis II reached its last stop before the Moon.&lt;/p&gt;
&lt;p&gt;KENNEDY SPACE CENTER, Florida—Preparations for the first human spaceflight to the Moon in more than 50 years took a big step forward this weekend with the rollout of the Artemis II rocket to its launch pad.&lt;/p&gt;
&lt;p&gt;The rocket reached a top speed of just 1 mph on the four-mile, 12-hour journey from the Vehicle Assembly Building to Launch Complex 39B at NASA’s Kennedy Space Center in Florida. At the end of its nearly 10-day tour through cislunar space, the Orion capsule on top of the rocket will exceed 25,000 mph as it plunges into the atmosphere to bring its four-person crew back to Earth.&lt;/p&gt;
&lt;p&gt;“This is the start of a very long journey,” said NASA Administrator Jared Isaacman. “We ended our last human exploration of the moon on Apollo 17.”&lt;/p&gt;
&lt;p&gt;The Artemis II mission will set several notable human spaceflight records. Astronauts Reid Wiseman, Victor Glover, Christina Koch, and Jeremy Hansen will travel farther from Earth than any human in history. They won’t land. That distinction will fall to the next mission in line in NASA’s Artemis program.&lt;/p&gt;
&lt;p&gt;But the Artemis II astronauts will travel more than 4,000 miles beyond the far side of the Moon (the exact distance depends on the launch date), setting up for a human spaceflight speed record during their blazing reentry over the Pacific Ocean a few days later. Koch will become the first woman to fly to the vicinity of the Moon, and Hansen will be the first non-US astronaut to do the same.&lt;/p&gt;
&lt;p&gt;“We really are ready to go,” said Wiseman, the Artemis II commander, during Saturday’s rollout to the launch pad. “We were in a sim [in Houston] for about 10 hours yesterday doing our final capstone entry and landing sim. We got in T-38s last night and we flew to the Cape to be here for this momentous occasion.”&lt;/p&gt;
&lt;p&gt;The rollout began around sunrise Saturday, with NASA’s Space Launch System rocket and Orion capsule riding a mobile launch platform and a diesel-powered crawler transporter along a throughway paved with crushed Alabama river rock. Employees, VIPs, and guests gathered along the crawlerway to watch the 11 million-pound stack inch toward the launch pad. The rollout concluded about an hour after sunset, when the crawler transporter’s jacking system lowered the mobile launch platform onto pedestals at Pad 39B.&lt;/p&gt;
&lt;p&gt;The rollout keeps the Artemis II mission on track for liftoff as soon as next month, when NASA has a handful of launch opportunities on February 6, 7, 8, 10, and 11.&lt;/p&gt;
&lt;p&gt;The big milestone leading up to launch day will be a practice countdown or Wet Dress Rehearsal (WDR), currently slated for around February 2, when NASA’s launch team will pump more than 750,000 gallons of super-cold liquid hydrogen and liquid oxygen into the rocket. NASA had trouble keeping the cryogenic fluids at the proper temperature, thenencountered hydrogen leakswhen the launch team first tried to fill the rocket for the unpiloted Artemis I mission in 2022. Engineers implemented the same fixes on Artemis II that they used to finally get over the hump with propellant loading on Artemis I.&lt;/p&gt;
&lt;p&gt;So, what are the odds NASA can actually get the Artemis II mission off the ground next month?&lt;/p&gt;
&lt;p&gt;“We’ll have to have things go right,” said Matt Ramsey, NASA’s Artemis II mission manager, in an interview with Ars on Saturday. “There’s a day of margin there for weather. There’s some time after WDR that we’ve got for data reviews and that sort of thing. It’s not unreasonable, but I do think it’s a success-oriented schedule.”&lt;/p&gt;
&lt;p&gt;The Moon has to be in the right position in its orbit for the Artemis II launch to proceed. There are also restrictions on launch dates to ensure the Orion capsule returns to Earth and reenters the atmosphere at an angle safe for theship’s heat shield. If the launch does not happen in February, NASA has a slate of backup launch dates in early March.&lt;/p&gt;
&lt;p&gt;Ars was at Kennedy Space Center for the rocket’s move to the launch pad Saturday. The photo gallery below shows the launcher emerging from the Vehicle Assembly Building, the same facility once used to stack Saturn V rockets during the Apollo Moon program. The Artemis II astronauts were also on hand for a question and answer session with reporters.&lt;/p&gt;
&lt;p&gt;The first flight of astronauts on the SLS rocket and Orion spacecraft is running at least five years late. The flight’s architecture, trajectory, and goals have changed multiple times, and technical snags discovered during manufacturing and testing repeatedly shifted the schedule. The program’sengineering and budgetary problems are well documented.&lt;/p&gt;
&lt;p&gt;But the team readying the rocket and spacecraft for launch has hit a stride in recent months. Technicians inside the Vehicle Assembly Building started stacking the SLS rocket in late 2024, beginning with the vehicle’s twin solid-fueled boosters. Then ground teams added the core stage, upper stage, and finally installed the Orion spacecraft on top of the rocket last October.&lt;/p&gt;
&lt;p&gt;Working nearly around the clock in three shifts, it took about 12 months for crews at Kennedy to assemble the rocket and prepare it for rollout. But the launch campaign inside the VAB was remarkably smooth. Ground teams shaved about two months off the time it took to integrate the SLS rocket and Orion spacecraft for the Artemis I mission, which launched on the program’s first full-up unpiloted test flight in 2022.&lt;/p&gt;
&lt;p&gt;“About a year ago, I was down here and we set the rollout date, and we hit it within a day or two,” said Matt Ramsey, NASA’s mission manager for Artemis II. “Being able to stay on schedule, it was a daily grind to be able to do that.”&lt;/p&gt;
&lt;p&gt;Engineers worked through a handful of technical problems last year, including an issue with a pressure-assisted device used to assist the astronauts in opening the Orion hatch in the event of an emergency. More recently, NASA teams cleared a concern with caps installed on the rocket’s upper stage, according to Ramsey.&lt;/p&gt;
&lt;p&gt;The most significant engineering review focused on proving the Orion heat shield is safe to fly. That assessment occurred in the background from the perspective of the technicians working on Artemis II at Kennedy.&lt;/p&gt;
&lt;p&gt;The Artemis II team is now focused on activities at the launch pad. This week, NASA plans to perform a series of tests extending and retracting the crew access mark. Next, the Artemis II astronauts will rehearse an emergency evacuation from the launch pad. That will be followed by servicing of the rocket’s hydraulic steering system.&lt;/p&gt;
&lt;p&gt;All of this leads up to the crucial practice countdown early next month. The astronauts won’t be aboard the rocket for the test, but almost everything else will look like launch day. The countdown will halt around 30 seconds prior to the simulated liftoff.&lt;/p&gt;
&lt;p&gt;It took repeated tries to get through the Wet Dress Rehearsal for the Artemis I mission. There were four attempts at the countdown practice run before the first actual Artemis I launch countdown. After encountering hydrogen leaks on two scrubbed launch attempts, NASA performed another fueling test before finally successfully launching Artemis I in November 2022.&lt;/p&gt;
&lt;p&gt;The launch team repaired a leaky hydrogen seal and introduced a gentler hydrogen loading procedure to overcome the problem. Hydrogen is an extremely efficient fuel for rockets, but its super-cold temperature and the tiny size of hydrogen molecules make it prone to leakage. The hydrogen feeds the SLS rocket’s four core stage engines and single upper stage engine.&lt;/p&gt;
&lt;p&gt;“Artemis I was a test flight, and we learned a lot during that campaign getting to launch,” said Charlie Blackwell-Thompson, NASA’s Artemis II launch director. “The things that we’ve learned relative to how to go load this vehicle, how to load LOX (liquid oxygen), how to load hydrogen, have all been rolled in to the way in which we intend to load the Artemis II vehicle.”&lt;/p&gt;
&lt;p&gt;NASA is hesitant to publicly set a target launch date until the agency gets through the dress rehearsal, but agency officials say a February launch remains feasible.&lt;/p&gt;
&lt;p&gt;“We’ve held schedule pretty well getting to rollout today,” Isaacman said. “We have zero intention of communicating an actual launch date until we get through wet dress. But look, that’s our first window, and if everything is tracking accordingly, I know the teams are prepared, I know this crew is prepared, we’ll take it.”&lt;/p&gt;
&lt;p&gt;“Wet dress is the driver to launch,” Blackwell-Thompson said. “With a wet dress that is without significant issues, if everything goes to plan, then certainly there are opportunities within February that could be achievable.”&lt;/p&gt;
&lt;p&gt;One constraint that threw a wrench into NASA’s Artemis I launch campaign is no longer a significant factor for Artemis II. On Artemis I, NASA had to roll the rocket back to the Vehicle Assembly Building (VAB) after the wet dress rehearsal to complete final installation and testing on its flight termination system, which consists of a series of pyrotechnic charges designed to destroy the rocket if it flies off course and threatens populated areas after liftoff.&lt;/p&gt;
&lt;p&gt;The US Space Force’s Eastern Range, responsible for public safety for all launches from Florida’s Space Coast, requires the flight termination system be retested after 28 to 35 days, a clock that started ticking last week before rollout. During Artemis I, technicians could not access the parts of the rocket they needed to in order to perform the retest at the launch pad. NASA now has structural arms to give ground teams the ability to reach parts higher up the rocket for the retest without returning to the hangar.&lt;/p&gt;
&lt;p&gt;With this new capability, Artemis II could remain at the pad for launch opportunities in February and March before officials need to bring it back to the VAB to replace the flight termination system’s batteries, which still can’t be accessed at the pad.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/space/2026/01/nasas-artemis-ii-rocket-rolls-to-launch-pad-but-key-test-looms-ahead/</guid><pubDate>Mon, 19 Jan 2026 13:01:52 +0000</pubDate></item></channel></rss>