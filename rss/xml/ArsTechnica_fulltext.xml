<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ArsTechnica Full-Text Feed</title><link>file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml</link><description>Full-text ArsTechnica articles (last 24 h)</description><atom:link href="file:///Users/purp/Downloads/translator/xml/ArsTechnica_fulltext.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 19 Jan 2026 17:10:33 +0000</lastBuildDate><item><title>10 things I learned from burning myself out with AI coding agents</title><link>https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/super-programmer-hes-heating-up-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Opinion: As software power tools, AI agents may make people busier than ever before.&lt;/p&gt;
&lt;p&gt;If you’ve ever used a 3D printer, you may recall the wondrous feeling when you first printed something you could have never sculpted or built yourself. Download a model file, load some plastic filament, push a button, and almost like magic, a three-dimensional object appears. But the result isn’t polished and ready for mass production, and creating a novel shape requires more skills than just pushing a button. Interestingly, today’sAI coding agentsfeel much the same way.&lt;/p&gt;
&lt;p&gt;Since November, I have usedClaude Codeand Claude Opus 4.5 through a personal Claude Max account to extensively experiment with AI-assisted software development (I have also used OpenAI’sCodexin a similar way, though not as frequently). Fifty projects later, I’ll be frank: I have not had this much fun with a computer since I learned BASIC on myApple II Pluswhen I was 9 years old. This opinion comes not as an endorsement but as personal experience: I voluntarily undertook this project, and I paid out of pocket for both OpenAI and Anthropic’s premium AI plans.&lt;/p&gt;
&lt;p&gt;Throughout my life, I have dabbled in programming as a utilitarian coder, writing small tools or scripts when needed. In my web development career, I wrote some small tools from scratch, but I primarily modified other people’s code for my needs. Since 1990, I’ve programmed in BASIC, C, Visual Basic, PHP, ASP, Perl, Python, Ruby, MUSHcode, and some others. I am not an expert in any of these languages—I learned just enough to get the job done. I have developed my own hobby games over the years using BASIC, Torque Game Engine, and Godot, so I have some idea of what makes a good architecture for a modular program that can be expanded over time.&lt;/p&gt;
&lt;p&gt;Claude Code, Codex, and Google’sGemini CLI, can seemingly perform software miracles on a small scale. They can spit out flashy prototypes of simple applications, user interfaces, and even games, but only as long as they borrow patterns from their training data. Much like a 3D printer, doing production-level work takes far more effort. Creating durable production code, managing a complex project, or crafting something truly novel still requires experience, patience, and skill beyond what today’s AI agents can provide on their own.&lt;/p&gt;
&lt;p&gt;And yet these tools have opened a world of creative potential in software that was previously closed to me, and they feel personally empowering. Even with that impression, though, I know these are hobby projects, and the limitations of coding agents lead me to believe that veteran software developers probably shouldn’t fear losing their jobs to these tools any time soon. In fact, they may become busier than ever.&lt;/p&gt;
&lt;p&gt;So far, I have created over 50 demo projects in the past two months, fueled in part by a bout of COVID that left me bedridden with a laptop and a generous 2x Claude usage cap that Anthropic put in place during the last few weeks of December. As I typed furiously all day, my wife kept asking me, “Who are you talking to?”&lt;/p&gt;
&lt;p&gt;You can see a few of the more interesting resultslistedon my personal website. Here are 10 interesting things I’ve learned from the process.&lt;/p&gt;
&lt;p&gt;Even with the best AI coding agents available today, humans remain essential to the software development process. Experienced human software developers bring judgment, creativity, and domain knowledge that AI models lack. They know how to architect systems for long-term maintainability, how to balance technical debt against feature velocity, and when to push back when requirements don’t make sense.&lt;/p&gt;
&lt;p&gt;For hobby projects like mine, I can get away with a lot of sloppiness. But for production work, having someone who understands version control, incremental backups, testing one feature at a time, and debugging complex interactions between systems makes all the difference. Knowing something about how good software development works helps a lot when guiding an AI coding agent—the tool amplifies your existing knowledge rather than replacing it.&lt;/p&gt;
&lt;p&gt;As independent AI researcher Simon Willisonwrotein a post distinguishing serious AI-assisted development from casual “vibe coding,” “AI tools amplify existing expertise. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.”&lt;/p&gt;
&lt;p&gt;With AI assistance, you don’t have to rememberhowto do everything. You just need to know what you want to do.&lt;/p&gt;
&lt;p&gt;So I like to remind myself that coding agents are software tools best used to enact human ideas, not autonomous coding employees. They arenot people(and not people replacements) no matter how the companies behind them might market them.&lt;/p&gt;
&lt;p&gt;If you think about it, everything you do on a computer was once a manual process. Programming a computer like theENIACinvolved literally making physical bits (connections) with wire on aplugboard. The history of programming has been one of increasing automation, so even though this AI-assisted leap is somewhat startling, one could think of these tools as an advancement similar to the advent of high-level languages, automated compilers and debugger tools, or GUI-based IDEs. They can automate many tasks, but managing the overarching project scope still falls to the person telling the tool what to do.&lt;/p&gt;
&lt;p&gt;And they can have rapidly compounding benefits. I’ve now used AI tools to write better tools—such as changing the source of an emulator so a coding agent can use it directly—and those improved tools are already having ripple effects. But a human must be in the loop for the best execution of my vision. This approach has kept me very busy, and contrary to some prevailing fears about peoplebecoming dumberdue to AI, I have learned many new things along the way.&lt;/p&gt;
&lt;p&gt;Like all AI models based on theTransformer architecture, the large language models (LLMs) that underpin today’s coding agents have a significant limitation: They can only reliably apply knowledge gleaned from training data, and they have a limited ability to generalize that knowledge to novel domains not represented in that data.&lt;/p&gt;
&lt;p&gt;What is training data? In this case, when building coding-flavored LLMs, AI companiesdownloadmillions of examples of software code from sources like GitHub and use them to make the AI models. Companies later specialize them for coding through fine-tuning processes.&lt;/p&gt;
&lt;p&gt;The ability of AI agents to use trial and error—attempting something and then trying again—helps mitigate the brittleness of LLMs somewhat. But it’s not perfect, and it can be frustrating to see a coding agent spin its wheels trying and failing at a task repeatedly, either because it doesn’t know how to do it or because it previously learned how to solve a problem but then forgot because the context window got compacted (more on thathere).&lt;/p&gt;
&lt;p&gt;To get around this, it helps to have the AI model take copious notes as it goes along about how it solved certain problems so that future instances of the agent can learn from them again. You also want to set ground rules in the claude.md file that the agent reads when it begins its session.&lt;/p&gt;
&lt;p&gt;This brittleness means that coding agents are almost frighteningly good at what they’ve been trained and fine-tuned on—modern programming languages, JavaScript, HTML, and similar well-represented technologies—and generally terrible at tasks on which they have not been deeply trained, such as 6502 Assembly or programming an Atari 800 game with authentic-looking character graphics.&lt;/p&gt;
&lt;p&gt;It took me five minutes to make a nice HTML5 demo with Claude but a week of torturous trial and error, plus actual systematic design on my part, to make a similar demo of an Atari 800 game. To do so, I had to use Claude Code to invent several tools, like command-line emulators andMCP servers, that allow it to peek into the operation of the Atari 800’s memory and chipset to even begin to make it happen.&lt;/p&gt;
&lt;p&gt;Due to what might poetically be called “preconceived notions” baked into a coding model’s neural network (more technically, statistical semantic associations), it can be difficult to get AI agents to create truly novel things, even if you carefully spell out what you want.&lt;/p&gt;
&lt;p&gt;For example, I spent four days trying to get Claude Code to create an Atari 800 version of my HTML gameViolent Checkers, but it had trouble because in the game’s design, the squares on the checkerboard don’t matter beyond their starting positions. No matter how many times I told the agent (and made notes in my Claude project files), it would come back to trying to center the pieces to the squares, snap them within squares, or use the squares as a logical basis of the game’s calculations when they should really just form a background image.&lt;/p&gt;
&lt;p&gt;To get around this in the Atari 800 version, I started over and told Claude that I was creating a game with a UFO (instead of a circular checker piece) flying over a field of adjacent squares—never once mentioning the words “checker,” “checkerboard,” or “checkers.” With that approach, I got the results I wanted.&lt;/p&gt;
&lt;p&gt;Why does this matter? Because with LLMs, context is everything, and in language, context changes meaning. Take the word “bank” and add the words “river” or “central” in front of it, and see how the meaning changes. In a way, words act as addresses thatunlockthe semantic relationships encoded in a neural network. So if you put “checkerboard” and “game” in the context, the model’sself-attention processlinks up a massive web of semantic associations about how checkers games should work, and that semantic baggage throws things off.&lt;/p&gt;
&lt;p&gt;A couple of tricks can help AI coders navigate around these limitations. First, avoid contaminating the context with irrelevant information. Second, when the agent gets stuck, try this prompt: “What information do you need that would let you implement this perfectly right now? What tools are available to you that you could use to discover that information systematically without guessing?” This forces the agent to identify (semantically link up) its own knowledge gaps, spelled out in the context window and subject to future action, instead of flailing around blindly.&lt;/p&gt;
&lt;p&gt;The first 90 percent of an AI coding project comes in fast and amazes you. The last 10 percent involves tediously filling in the details through back-and-forth trial-and-error conversation with the agent. Tasks that require deeper insight or understanding than what the agent can provide still require humans to make the connections and guide it in the right direction. The limitations we discussed above can also cause your project to hit a brick wall.&lt;/p&gt;
&lt;p&gt;From what I have observed over the years, larger LLMs can potentially make deeper contextual connections than smaller ones. They havemore parameters(encoded data points), and those parameters are linked in more multidimensional ways, so they tend to have a deeper map of semantic relationships. As deep as those go, it seems that human brains still have an even deeper grasp of semantic connections and can make wild semantic jumps that LLMs tend not to.&lt;/p&gt;
&lt;p&gt;Creativity, in this sense, may be when you jump from, say, basketball to how bubbles form in soap film and somehow make a useful connection that leads to a breakthrough. Instead, LLMs tend to follow conventional semantic paths that are more conservative and entirely guided by mapped-out relationships from the training data. That limits their creative potential unless the prompter unlocks it by guiding the LLM to make novel semantic connections. That takes skill and creativity on the part of the operator, which once again shows the role of LLMs as tools used by humans rather than independent thinking machines.&lt;/p&gt;
&lt;p&gt;While creating software with AI coding tools, the joy of experiencing novelty makes you want to keep adding interesting new features rather than fixing bugs or perfecting existing systems. And Claude (or Codex) is happy to oblige, churning away at new ideas that are easy to sketch out in a quick and pleasing demo (the 90 percent problem again) rather than polishing the code.&lt;/p&gt;
&lt;p&gt;Fixing bugs can also create bugs elsewhere. This is not new to coding agents—it’s a time-honored problem in software development. But agents supercharge this phenomenon because they can barrel through your code and make sweeping changes in pursuit of narrow-minded goals that affect lots of working systems. We’ve already talked about the importance of having a good architecture guided by the human mind behind the wheel above, and that comes into play here.&lt;/p&gt;
&lt;p&gt;Given the limitations I’ve described above, it’s very clear that an AI model with general intelligence—what people usually callartificial general intelligence(AGI)—is still not here. AGI would hypothetically be able to navigate around baked-in stereotype associations and not have to rely on explicit training or fine-tuning on many examples to get things right. AI companies will probably need a different architecture in the future.&lt;/p&gt;
&lt;p&gt;I’m speculating, but AGI would likely need to learn permanently on the fly—as in modify its own neural network weights—instead of relying on what is called “in-context learning,” which only persists until the context fills up and gets compacted or wiped out.&lt;/p&gt;
&lt;p&gt;In other words, you could teach a true AGI system how to do something by explanation or let it learn by doing, noting successes, and having those lessons permanently stick, no matter what is in the context window. Today’s coding agents can’t do that—they forget lessons from earlier in a long session or between sessions unless you manually document everything for them. My favorite trick is instructing them to write a long, detailed report on what happened when a bug is fixed. That way, you can point to the hard-earned solution the next time the amnestic AI model makes the same mistake.&lt;/p&gt;
&lt;p&gt;While using Claude Code for a while, it’s easy to take for granted that you suddenly have the power to create software without knowing certain programming languages. This is amazing at first, but you can quickly become frustrated that what is conventionally a very fast development process isn’t fast enough. Impatience at the coding machine sets in, and you start wanting more.&lt;/p&gt;
&lt;p&gt;But even if you do know the programming languages being used, you don’t get a free pass. You still need to make key decisions about how the project will unfold. And when the agent gets stuck or makes a mess of things, your programming knowledge becomes essential for diagnosing what went wrong and steering it back on course.&lt;/p&gt;
&lt;p&gt;After guiding way too many hobby projects through Claude Code over the past two months, I’m starting to think that most people won’t become unemployed due to AI—they will become busier than ever. Power tools allow more work to be done in less time, and the economy will demand more productivity to match.&lt;/p&gt;
&lt;p&gt;It’s almost too easy to make new software, in fact, and that can be exhausting. One project idea would lead to another, and I was soon spending eight hours a day during my winter vacation shepherding about 15 Claude Code projects at once. That’s too much split attention for good results, but the novelty of seeing my ideas come to life was addictive. In addition to the game ideas I’ve mentioned here, I made tools that scrape and search my past articles, a graphical MUD based on ZZT, a new type ofMUSH(text game) that uses AI-generated rooms, a new type ofTelnet display proxy, and a Claude Code client for the Apple II (more on that soon). I also put two AI-enabled emulators forApple IIandAtari 800on GitHub. Phew.&lt;/p&gt;
&lt;p&gt;Consider the advent of thesteam shovel, which allowed humans to dig holes faster than a team using hand shovels. It made existing projects faster and new projects possible. But think about the human operator of the steam shovel. Suddenly, we had a tireless tool that could work 24 hours a day if fueled up and maintained properly, while the human piloting it would need to eat, sleep, and rest.&lt;/p&gt;
&lt;p&gt;In fact, we may end up needing new protections for human knowledge workers using these tireless information engines to implement their ideas, much as unions rose as a response to industrial production lines over 100 years ago. Humans need rest, even when machines don’t.&lt;/p&gt;
&lt;p&gt;Will an AI system ever replace the human role here? Even if AI coding agents could eventually work fully autonomously, I don’t think they’ll replace humans entirely because there will still be people who want to get things done, and new AI power tools will emerge to help them do it.&lt;/p&gt;
&lt;p&gt;AI coding tools can turn what was once a year-long personal project into a five-minute session. I fed Claude Code a photo of a two-player Tetris game I sketched in a notebook back in 2008, and it produced aworking prototypein minutes (prompt: “create a fully-featured web game with sound effects based on this diagram”). That’s wild, and even though the results are imperfect, it’s a bit frightening to comprehend what kind of sea change in software development this might entail.&lt;/p&gt;
&lt;p&gt;Since early December, I’ve beenpostingsome of my more amusing experimental AI-coded projects to Bluesky for people to try out, but I discovered I needed to deliberately slow down with updates because they came too fast for people to absorb (and too fast for me to fully test). I’ve also received comments like “I’m worried you’re using AI, you’re making games too fast” and so on.&lt;/p&gt;
&lt;p&gt;Regardless of my own habits, the flow of new software will not slow down. There will soon be a seemingly endless supply of AI-augmented media (games, movies, images, books), and that’s a problem we’ll have to figure out how to deal with. These products won’t all be “AI slop,” either; some will be done very well, and the acceleration in production times due to these new power tools will balloon the quantity beyond anything we’ve seen.&lt;/p&gt;
&lt;p&gt;Social media tends to prime people to believe that AI is all good or all bad, but that kind of black-and-white thinking may be the easy way out. You’ll have no cognitive dissonance, but you’ll miss a far richer third option: seeing these tools as imperfect and deserving of critique but also as useful and empowering when they bring your ideas to life.&lt;/p&gt;
&lt;p&gt;AI agents should be considered tools, not entities or employees, and they should be amplifiers of human ideas. My game-in-progressCard Mineris entirely my own high-level creative design work, but the AI model handled the low-level code. I am still proud of it as an expression of my personal ideas, and it would not exist without AI coding agents.&lt;/p&gt;
&lt;p&gt;For now, at least, coding agents remain very much tools in the hands of people who want to build things. The question is whether humans will learn to wield these new tools effectively to empower themselves. Based on two months of intensive experimentation, I’d say the answer is a qualified yes, with plenty of caveats.&lt;/p&gt;
&lt;p&gt;We also have social issues to face: Professional developers already use these tools, and with the prevailing stigma against AI tools in some online communities, many software developers and the platforms that host their work will face difficult decisions.&lt;/p&gt;
&lt;p&gt;Ultimately, I don’t think AI tools will make human software designers obsolete. Instead, they may well help those designers become more capable. This isn’t new, of course; tools of every kind have been serving this role since long before the dawn of recorded history. The best tools amplify human capability while keeping a person behind the wheel. The 3D printer analogy holds: amazing fast results are possible, but mastery still takes time, skill, and a lot of patience with the machine.&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/</guid><pubDate>Mon, 19 Jan 2026 03:00:45 +0000</pubDate></item><item><title>Meet Veronika, the tool-using cow</title><link>https://arstechnica.com/science/2026/01/meet-veronika-the-tool-using-cow/</link><description>&lt;![CDATA[
&lt;p&gt;&lt;img src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/cowtool1-1152x648.jpg" alt="Article image"/&gt;&lt;/p&gt;
&lt;p&gt;Veronika uses sticks to scratch herself, suggesting scientists have underestimated cow cognition&lt;/p&gt;
&lt;p&gt;Far Sidefans might recall a classic 1982 cartoon called “Cow Tools,” featuring a cow standing next to a jumble of strange objects—the joke being that cows don’t use tools. That’s why a pet Swiss brown cow in Austria named Veronika has caused a bit of a sensation: she likes to pick up random sticks and use them to scratch herself. According to anew paperpublished in the journal Current Biology, this is a form of multipurpose tool use and suggests that the cognitive capabilities of cows have been underestimated by scientists.&lt;/p&gt;
&lt;p&gt;Aspreviously reported, tool use was once thought to be one of the defining features of humans, but examples of it were eventually observed in primates and other mammals. Dolphins can toss objects as a form of play which some scientists consider to be a type of tool use, particularly when it involves another member of the same species. Potential purposes include a means of communication, social bonding, or aggressiveness. (Octopuses have alsobeen observedengaging in similar throwing behavior.)&lt;/p&gt;
&lt;p&gt;But the biggest surprise came when birds were observed using tools in the wild. After all, birds are the only surviving dinosaurs, and mammals and dinosaurs hadn’t shared a common ancestor for hundreds of millions of years. In the wild, observedtool use has been limitedto the corvids (crows and jays), which show a variety of other complex behaviors—they’llremember your faceand recognize thepassing of their dead.&lt;/p&gt;
&lt;p&gt;In 2012, a captive cockatoo named Figaro was trying to retrieve a stone that had fallen behind a metal divider and picked up a stick to increase his reach. He failed but then the researchers placed a nut behind the divider as an added incentive. Figaro initially picked up a stick from the enclosure’s floor, but this proved to be too short to reach the food. So, he actually splintered off a piece of the enclosure’s wooden base, and successfully used that to pull the nut towards the wire until he could use his beak to grab it.&lt;/p&gt;
&lt;p&gt;Figaro used a different tool in each subsequent trial, and in most cases made some modifications to it before successfully retrieving a nut. In at least one case, he performed four separate modifications before putting a stick to use in retrieving the nut. He also managed to use the tool in two different ways, often alternating dragging and sweeping motions in his efforts to pull the food within reach. Other cockatoos who repeatedly watched Figaro’s performance were also able to do so. By 2022, Figaro and his cockatoo cronieshad learnedhow tocombine tools—in this case, a stick and a ball—to play a rudimentary form of “golf.”&lt;/p&gt;
&lt;p&gt;For all the scientific interest in animal tool use, there has been almost no research into the cognitive capabilities of livestock like cows, or their possible tool use, according to the authors of this latest paper. Alice Auersperg, a cognitive biologist at the University of Veterinary Medicine in Vienna, was intrigued when she watched the footage of Veronika’s scratching behavior. “It was clear that this was not accidental,”said Auersperg. “This was a meaningful example of tool use.”&lt;/p&gt;
&lt;p&gt;To test the extent of Veronika’s tool use, Auersperg and her postdoc, Antonio Osuna-Mascaro, visited the farm where Veronika lives. Her owner, Witgar Wiegele, is an organic farmer and baker who keeps the cow as a pet. With Wiegele’s permission, they conducted a series of randomized trials with a deck scrub broom, chosen for its asymmetrical shape, placed in different orientations. The pair recorded 76 instances of Veronika using the broom over seven sessions of ten trials.&lt;/p&gt;
&lt;p&gt;Each time, Veronika used her tongue to lift and position the broom in her mouth, clamping down with her teeth for a stable grip. This enabled her to use the broom to scratch otherwise hard-to-reach areas on the rear half of her body. Veronika seemed to prefer the brush end to the stick end (i.e., the exploitation of distinct properties of a single object for different functions) although which end she used depended on body area. For example, she used the brush end to scratch her upper body using a scrubbing motion, while using the stick end to scratch more sensitive lower areas like her udders and belly skin flaps using precisely targeted gentle forward pushes. She also anticipated the need to adjust her grip.&lt;/p&gt;
&lt;p&gt;The authors conclude that this behavior demonstrates “goal-directed, context-sensitive tooling,” as well as versatility in her tool-use anticipation, and fine-motor targeting. Veronika’s scratching behavior is likely motivated by the desire to relieve itching from insect bites, but her open, complex environment, compared to most livestock, and regular interactions with humans enabled her unusual cognitive abilities to emerge.&lt;/p&gt;
&lt;p&gt;The implication is that this kind of technical problem-solving is not confined to species with large brains and hands or beaks. “[Veronika] did not fashion tools like the cow in Gary Larson’s cartoon, but she selected, adjusted, and used one with notable dexterity and flexibility,” the authors wrote. “Perhaps the real absurdity lies not in imagining a tool-using cow, but in assuming such a thing could never exist.”&lt;/p&gt;
&lt;p&gt;DOI: Current Biology, 2025.10.1016/j.cub.2025.11.059(About DOIs).&lt;/p&gt;
]]&gt;</description><guid isPermaLink="false">https://arstechnica.com/science/2026/01/meet-veronika-the-tool-using-cow/</guid><pubDate>Mon, 19 Jan 2026 07:00:27 +0000</pubDate></item></channel></rss>